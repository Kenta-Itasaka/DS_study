{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DS_study.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"12q-DrfyNvGAC0wmYJRCyipJyzkDGrmyJ","authorship_tag":"ABX9TyOey53yhX0oFSIkslbGZXTa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"hEHgwQ6LagMw"},"source":["!pip install https://github.com/ipython-contrib/jupyter_contrib_nbextensions/tarball/master\n","!jupyter contrib nbextension install --user\n","!jupyter nbextension enable hinterland/hinterland"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXjhZnHY9Yj9"},"source":["import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ZL6T59FeIcn"},"source":["# **分析コンペにおけるタスクの種類**"]},{"cell_type":"markdown","metadata":{"id":"cUVGOg3giL_p"},"source":["## **回帰タスク**\n","\n","---\n","\n","* 数値を予測するのが回帰タスク.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2kAXup7KiDt0"},"source":["## **分類タスク**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"gLi_ucIIsJff"},"source":["### **二値分類**\n","- レコードがある属性に属しているかどうかを予測するのが分類タスク."]},{"cell_type":"markdown","metadata":{"id":"svRRN6zJsJqH"},"source":["### **多クラス分類**\n","- レコードが複数のクラスのうちどれか一つに属している場合はマルチクラス分類.\n"," - 分析で使用される主なモデルはマルチクラス分類に対応している.\n","- レコードが同時に複数のクラスに属する可能性がある場合はマルチラベル分類.\n"," - マルチラベル分類では二値分類をクラスの数だけ繰り返すのが基本的な解法.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0jTyexEQqvmQ"},"source":["# **評価指標**"]},{"cell_type":"markdown","metadata":{"id":"zWwQlbvKq5LU"},"source":["## **回帰における評価指標**\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1XVsxXXGglfT"},"source":["### **RMSE (Root Mean Squared Error)**\n","回帰タスクで最も代表的な評価指標. 誤差の $l^2$ ノルム.\n","\\begin{align*}\n","\\mathrm{RMSE} = \\sqrt{\\frac{1}{N} \\sum^{N}_{i = 1}(y_{i} - \\hat{y}_{i})^2}\n","\\end{align*}\n","\n","- 誤差が正規分布に従う場合は, 最小二乗解 $\\Leftrightarrow$ 最尤解.\n","実際, $y_i - \\hat{y}_i \\sim N(0, \\sigma)$ ならば, \n","\\begin{align*}\n","l(y-\\hat{y}) = - \\log L(y-\\hat{y}) = - \\frac{N}{2} \\log(2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2}\\sum^N_{i=1}(y_i - \\hat{y}_i)^2.\n","\\end{align*}\n","\n","- MAE (誤差の $l^1$ ノルム) と比較すると外れ値の影響を受けやすい. 外れ値を除く等の処理をしておかないと外れ値に過剰に適合したモデルを作成してしまう可能性がある.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sykghyrn7qIh","executionInfo":{"status":"ok","timestamp":1627000079747,"user_tz":-540,"elapsed":389,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"50ecb674-c3c1-4508-bd33-80c89a044eef"},"source":["from sklearn.metrics import mean_squared_error\n","\n","y_true = [1.0, 1.5, 2.0, 1.2, 1.8]\n","y_pred = [0.8, 1.5, 1.8, 1.3, 3.0]\n","\n","rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n","print(rmse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.5531726674375732\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oFt0iyklpdLK"},"source":["### **RMSLE (Root Mean Squared Logarithmic Error)**\n","対数での誤差の $l^2$ ノルム.\n","\n","\\begin{align*}\n","\\mathrm{RMSLE} = \\sqrt{\\frac{1}{N} \\sum^{N}_{i = 1} (\\log(1 + y_{i}) - \\log(1 + \\hat{y}_{i}))^2}\n","\\end{align*}\n","\n","- 目的変数が裾の重い分布を持ち変換しないままだと大きな値の影響が強い場合や, 真の値と予測値の比率に着目したい場合に用いられる.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MXBTB2buu8FJ","executionInfo":{"status":"ok","timestamp":1627000082986,"user_tz":-540,"elapsed":16,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"ab76de09-0969-4f5e-a598-f7ec3d4276be"},"source":["from sklearn.metrics import mean_squared_log_error\n","\n","y_true = [1.0, 1.5, 2.0, 1.2, 1.8]\n","y_pred = [0.8, 1.5, 1.8, 1.3, 3.0]\n","\n","rmsle = np.sqrt(mean_squared_log_error(y_true, y_pred))\n","print(rmsle)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.17032547044118185\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qh5Txx9_uv_V"},"source":["### **MAE (Mean Absolute Error)**\n","誤差の $l^1$ ノルム.\n","\n","\\begin{align*}\n","\\mathrm{MAE} = \\frac{1}{N} \\sum^{N}_{i = 1} |y_i - \\hat{y}_i|\n","\\end{align*}\n","\n","- 外れ値の影響を低減した形での評価に適している.\n","\n","- $\\hat{y}_i = y_i$ で $\\hat{y}_i$ について微分不可能であったり, 2階微分が常に $0$ となるという扱いづらい性質を持っている.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgkYbm-ru3-h","executionInfo":{"status":"ok","timestamp":1627001489779,"user_tz":-540,"elapsed":402,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"8d394cc8-7ccc-4c6d-a6d4-882d74d847f2"},"source":["from sklearn.metrics import mean_absolute_error\n","\n","y_true = [1.0, 1.5, 2.0, 1.2, 1.8]\n","y_pred = [0.8, 1.5, 1.8, 1.3, 3.0]\n","\n","mse = np.sqrt(mean_absolute_error(y_true, y_pred))\n","print(mse)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.58309518948453\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rxIKLWou7UB3"},"source":["### **決定係数 ($\\mathrm{R}^2$)**\n","回帰分析の当てはまりの良さを表す指標.\n","\\begin{align*}\n","& \\mathrm{R}^2 = 1 - \\frac{\\sum^{N}_{i=1} (y_i - \\hat{y}_i)^2}{\\sum^{N}_{i=1} (y_i - \\bar{y})^2} \\\\\n","& \\bar{y} = \\frac{1}{N} \\sum^{N}_{i = 1} y_i\n","\\end{align*}\n","\n","\n","- 最大で $1$ をとり, $1$ に近づくほど精度の高い予測になっている.\n","\n","- 分母は予測値に依らず, 分子は二乗誤差であるため, $\\mathrm{R}^2$ を最大化することは $\\mathrm{RMSE}$ を最小化することと同値.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJOpcgOC2IX-","executionInfo":{"status":"ok","timestamp":1627004050087,"user_tz":-540,"elapsed":387,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"bafaecac-cf61-4332-a8f9-a666b1c92c83"},"source":["from sklearn.metrics import r2_score\n","\n","y_true = [1.0, 1.5, 2.0, 1.2, 1.8]\n","y_pred = [0.8, 1.5, 1.8, 1.3, 2.0]\n","\n","r2 = r2_score(y_true, y_pred)\n","print(r2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8088235294117648\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"THbxCYjJDAn5"},"source":["## **二値分類における評価指標～正例か負例かを予測値とする場合～**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"pYVzwWu1DnWZ"},"source":["### **混同行列 (confusion matrix)**\n","\n","予測値を正例としたか負例としたか, 予測が正しいか誤りかによって, レコードを以下の4つのグループに分け, それぞれのレコード数を行列表示したもの.\n","\n","- TP (True Positive, 真陽性)：予測値を正例として, その予測が正しい場合\n","\n","- TN (True Negative, 真陰性)：予測値を負例として, その予測が正しい場合\n","\n","- FP (False Positive, 偽陽性)：予測値を正例として, その予測が誤りの場合\n","\n","- FN (False Negative, 偽陰性)：予測値を負例として, その予測が誤りの場合"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kms3wjGA_LnO","executionInfo":{"status":"ok","timestamp":1627026114082,"user_tz":-540,"elapsed":369,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"bf7989f7-4630-43c8-9940-f79adc145eff"},"source":["from sklearn.metrics import confusion_matrix\n","\n","y_true = [0, 1, 1, 1, 1, 0, 0, 0, 0, 1]\n","y_pred = [0, 0, 0, 1, 1, 0, 1, 0, 0, 1]\n","labels = list(set(y_true) | set(y_pred))\n","if len(labels) > 2:\n","  labels.sort(reverse=False)\n","else:\n","  labels.sort(reverse=True)\n","\n","confusion_matrix = confusion_matrix(y_true, y_pred, labels = labels).T\n","print(confusion_matrix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[3 1]\n"," [2 4]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I5JhdBM1Xdha"},"source":["### **accuracy (正答率) と error rate (誤答率)**\n","\n","accuracy は予測が正しい割合を表す指標, error rate は予測が誤っている割合を表す指標.\n","\n","\\begin{align*}\n","& \\mathrm{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} \\\\\n","& \\mathrm{error \\ rate} = 1 - \\mathrm{accuracy}\n","\\end{align*}\n","\n","- 不均衡なデータ, すなわち目的変数のクラスの割合が均一でないデータの場合には, モデルの性能を評価しづらい."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d0Inmhh-Mjb0","executionInfo":{"status":"ok","timestamp":1627028046148,"user_tz":-540,"elapsed":245,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"701c7488-fbfe-4e32-bd38-0e0c219648b1"},"source":["from sklearn.metrics import accuracy_score\n","\n","y_true = [1, 0, 1, 1, 0, 1, 1, 0]\n","y_pred = [0, 0, 1, 1, 0, 0, 1, 1]\n","\n","accuracy = accuracy_score(y_true, y_pred)\n","print(accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.625\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Dyg-PGWcPBE"},"source":["### **precision (適合率) と recall (再現率)**\n","precision は正例と予測したもののうち真の値も正例であるものの割合を表す指標, recall は真の値が正例のもののうち正例と予測されたものの割合を表す指標.\n","\n","\\begin{align*}\n","& \\mathrm{precision} = \\frac{TP}{TP + FP}\\\\\n","& \\mathrm{recall} = \\frac{TP}{TP + FN}\n","\\end{align*}\n","\n","- precision と recall は互いにトレードオフの関係になっており, どちらかの値を高くしようとすると, もう一方の値は低くなる.\n","\n","- 誤検知を少なくしたい場合は precision を重視し, 正例の見逃しを避けたい場合は recall を重視することになる."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nLK1wzrxbEyt","executionInfo":{"status":"ok","timestamp":1627030545400,"user_tz":-540,"elapsed":248,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"391db876-c9d7-4f95-9669-80003a2ccc15"},"source":["from sklearn.metrics import precision_score, recall_score\n","\n","y_true = [1, 0, 1, 1, 0, 1, 1, 0]\n","y_pred = [0, 0, 1, 1, 0, 0, 1, 1]\n","\n","precision = precision_score(y_true, y_pred)\n","recall = recall_score(y_true, y_pred)\n","print(precision)\n","print(recall)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.75\n","0.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LajHajcwnBuS"},"source":["### **F1-score と Fβ-score**\n","\n","F1-score は precision と recall の調和平均で計算される指標, Fβ-score は F1-score から recall と precision のバランスを, recall をどれだけ重視するかを表す係数βによって調整した指標.\n","\n","\\begin{align*}\n","& F_1 = \\frac{2}{\\frac{1}{\\mathrm{recall}} + \\frac{1}{\\mathrm{precision}}} = \\frac{2 \\cdot \\mathrm{recall} \\cdot \\mathrm{precision}}{\\mathrm{recall} + \\mathrm{precision}} = \\frac{2TP}{2TP + FP + FN} \\\\\n","& F_{\\beta} = \\frac{(1 + \\beta^2)}{\\frac{\\beta^2}{\\mathrm{recall}} + \\frac{1}{\\mathrm{precision}}} = \\frac{(1 + \\beta^2) \\cdot \\mathrm{recall} \\cdot \\mathrm{precision}}{\\mathrm{recall} + \\beta^2 \\mathrm{precision}} = \\frac{(1 + \\beta^2)TP}{(1 + \\beta^2)TP + FP + \\beta^2FN}\n","\\end{align*}\n","\n","- 分子に $TP$ のみが含まれることから分かるように, 正例と負例について対称ではなく, 真の値と予測値の正例と負例を入れ替えるとスコアやその振る舞いが変わる."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09RKdBMDkfyj","executionInfo":{"status":"ok","timestamp":1627033017560,"user_tz":-540,"elapsed":259,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"0c36fc9d-bb18-4a1f-e4e6-e016add12d56"},"source":["from sklearn.metrics import f1_score, fbeta_score\n","\n","y_true = [1, 0, 1, 1, 0, 1, 1, 0]\n","y_pred = [0, 0, 1, 1, 0, 0, 1, 1]\n","beta = 2.0\n","\n","f1 = f1_score(y_true, y_pred)\n","fbeta = fbeta_score(y_true, y_pred, beta=beta)\n","\n","print(f1)\n","print(fbeta)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6666666666666665\n","0.625\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XnPggwsYdddn"},"source":["## **二値分類における評価指標～正例である確率を予測値とする場合～**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"U-oI3azsdlRh"},"source":["### **logloss**\n","\n","分類タスクでの代表的な評価指標. cross entropy と呼ばれることもある.\n","\n","\\begin{align*}\n","\\mathrm{logloss} = -\\frac{1}{N} \\sum^{N}_{i=1}(y_i \\log p_i + (1 - y_i) \\log (1 - p_i))\n","\\end{align*}\n","\n","ここで, $y_i$ は正例かどうかを表すラベル (正例が $1$, 負例が $0$) を, $p_i$ は各レコードが正例である予測確率を表す.\n"]},{"cell_type":"code","metadata":{"id":"FDFwz8T0uHI5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627049433249,"user_tz":-540,"elapsed":373,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"e88288b2-9eeb-4f0f-eaaf-7e6cc3729d7b"},"source":["from sklearn.metrics import log_loss\n","\n","y_true = [1, 0, 1, 1, 0, 1]\n","y_pred = [0.1, 0.2, 0.8, 0.8, 0.1, 0.3]\n","\n","logloss = log_loss(y_true, y_pred)\n","print(logloss)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.7135581778200728\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x5cZeWCatE7I"},"source":["### **AUC (Area Under the ROC Curve)**\n","\n","AUC は ROC 曲線の下部の面積で表される指標. ROC 曲線は, 予測値を正例とする閾値を1から0に動かし, そのときの偽陽性率, 真陽性率を (x, y) としてプロットすることで定義される. \n","\n","- 偽陽性率：$\\frac{FP}{FP + TN} = \\frac{1}{1 + \\frac{TN}{FP}}$ \n","\n","- 真陽性率：$\\frac{TP}{TP + FN} = \\frac{1}{1 + \\frac{FN}{TP}}$\n","\n","- 偽陽性率, 真陽性率ともに, 閾値に対して広義単調減少.\n","\n","- 閾値が1で (偽陽性率, 真陽性率) = (0.0, 0.0), 閾値が0で (偽陽性率, 真陽性率) = (1.0, 1.0).\n","\n","- 完全な予測を行った場合には, ROC 曲線は (偽陽性率, 真陽性率) = (0.0, 1.0) の点を通り, AUC は1.0となる. ランダムな予測の場合は, 偽陽性率 = 真陽性率 = 1.0 - 閾値 と考えられ, AUC は0.5程度となる.\n","\n","- 予測値を反対にした場合 (すなわち, 1.0 - 元の予測値 とした場合) は, AUC は 1.0 - 元のAUC となる.\n"," - $\\tilde{\\cdot}$ で新しい値を表すと, \n","\\begin{align*}\n","& \\frac{\\tilde{FP}}{\\tilde{FP} + \\tilde{TN}} = \\frac{TN}{TN + FP} = 1.0 - \\frac{FP}{FP + TN} \\\\\n","& \\frac{\\tilde{TP}}{\\tilde{TP} + \\tilde{FN}} = \\frac{FN}{FN + TP} = 1.0 - \\frac{TP}{TP + FN}.\n","\\end{align*}\n","\n","- AUC は正例と負例をそれぞれ独立にランダムサンプリングしたときに正例の予測値が負例の予測値より大きい確率で近似することができる. \n","これを確認する.\n","偽陽性率, 真陽性率ともに, 閾値に対して狭義単調減少であると仮定する.\n","予測確率 $p$ が連続の値であるから, 自然な仮定である.\n","正例と負例の条件下における分布関数を\n","\\begin{align*}\n","& F_0(s) = P(p \\le s | y = 0) \\\\\n","& F_1(s) = P(p \\le s | y = 1)\n","\\end{align*}\n","で定義し, $f_0(s)$, $f_1(s)$ をそれぞれの密度関数とする. ここで, $s$ は閾値である. 偽陽性率を $x$, 新陽性率を $y$ で表すと, それぞれ\n","\\begin{align*}\n","& x(s) = P(p > s | y = 0) = 1 - F_0(s) \\\\\n","& y(s) = P(p > s | y = 1) = 1 - F_1(s) \n","\\end{align*}\n","と書ける. 仮定より, $x(s)$, $y(s)$ は $s$ について狭義単調減少であるから, \n","変数変換を行って, \n","\\begin{align*}\n","\\mathrm{AUC} & = \\int^{1}_{0} y(x) dx \\\\\n","& = \\int^{1}_{0} y(s) x^{\\prime}(s) ds \\\\\n","& = \\int^{1}_{0} (1 - F_1(s)) f_0(s) ds \\\\\n","\\end{align*}\n","を得る. \n","\\begin{align*}\n","1 - F_1(s) = \\int^{1}_{s} f_1(t) dt\n","\\end{align*}\n","に注意すると, \n","\\begin{align*}\n","\\int^{1}_{0} \\left( \\int^{1}_{s} f_1(t) dt \\right) f_0(s) ds \n","& = \\int^{1}_{0} \\int^{1}_{0} \\chi_{\\{ s \\le t \\}}(s, t) f_0(s) f_1(t) ds dt \\\\\n","& = P_{f_0, f_1}(S \\le T)\n","\\end{align*}\n","が分かる. したがって, \n","\\begin{align*}\n","AUC = P_{f_0, f_1}(S \\le T) \\simeq \\frac{\\# \\{ y^{0}_i = 0, y^{1}_j = 1, s_i \\le t_j \\}}{\\# \\{ y^{0}_i = 0, y^{1}_j = 1 \\}}.\n","\\end{align*}\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8DN-u_mtBVW","executionInfo":{"status":"ok","timestamp":1627111187197,"user_tz":-540,"elapsed":220,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"58bf690a-494e-480d-d74b-9a34f6921816"},"source":["from sklearn.metrics import roc_auc_score\n","\n","y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n","y_pred = np.array([0.2, 0.3, 0.6, 0.8, 0.4, 0.5, 0.7, 0.9])\n","\n","auc = roc_auc_score(y_true, y_pred)\n","print(auc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6875\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0s895eGcZpv4"},"source":["## **多クラス分類における評価指標**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"f3hTM0peZzCz"},"source":["### **multi-class accuracy**\n","\n","二値分類の accuracy を多クラスへ拡張したもので, 予測が正しい割合を表す指標. 予測が正解であるレコード数をすべてのレコード数で割った値として定義される.\n","\\begin{align*}\n","\\mathrm{multiclass \\ accuracy} = \\frac{\\# \\{ y_i = \\hat{y}_i \\} }{N}\n","\\end{align*}"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRmqCD3BYmC4","executionInfo":{"status":"ok","timestamp":1627112084954,"user_tz":-540,"elapsed":240,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"45ea5208-f626-4260-fbb7-3a31f29eef9e"},"source":["from sklearn.metrics import accuracy_score\n","\n","y_true = [1, 0, 1, 1, 2, 1, 1, 0, 2, 3]\n","y_pred = [0, 0, 1, 1, 2, 0, 1, 1, 1, 3]\n","\n","accuracy = accuracy_score(y_true, y_pred)\n","print(accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z0RgU_ekdU8y"},"source":["### **multi-class logloss**\n","\n","logloss をマルチクラス分類に拡張した指標. \n","\\begin{align*}\n","\\mathrm{multiclass \\ logloss} = - \\frac{1}{N} \\sum^{N}_{i=1} \\sum^{M}_{m=1} y_{i, m} \\log p_{i, m}\n","\\end{align*}\n","$M$ はクラス数. $y_{i, m}$ はレコード $i$ がクラス $m$ に属する場合は1, 属さない場合は0となる. $p_{i, m}$ はレコード $i$ がクラス $m$ に属する予測確率."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Wor3xbfcBN7","executionInfo":{"status":"ok","timestamp":1627113113366,"user_tz":-540,"elapsed":5,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"2a301970-d62b-4776-cbc4-2a707da55a7a"},"source":["from sklearn.metrics import log_loss\n","\n","y_true = [0, 2, 1, 2, 2]\n","y_pred = [[0.68, 0.32, 0.00],\n","          [0.00, 0.00, 1.00],\n","          [0.60, 0.40, 0.00],\n","          [0.00, 0.00, 1.00],\n","          [0.28, 0.12, 0.60]]\n","\n","logloss = log_loss(y_true, y_pred)\n","print(logloss)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.3625557672904274\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eMNwQ_NmlGIQ"},"source":["### **mean-F1 と macro-F1 と micro-F1**\n","\n","それぞれ, F1-score を多クラス分類に拡張したもの. 主にマルチラベル分類で用いられる.\n","\n","- mean-F1：レコード単位で F1-score を計算し, その行方向の平均値を評価指標のスコアとする.\n","\n","- macro-F1：クラス単位で F1-score を計算し, その列方向の平均値を評価指標のスコアとする. この指標は, それぞれのクラスで二値分類を行い, その F1-score を平均しているのと同じため, 各クラスで独立に閾値を最適化することができる.\n","\n","- micro-F1：レコード×クラスのベクトルを1次元ベクトルに変換し, そのベクトルに対する二値分類用の F1-score を評価指標とする.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abf1BZjOf7j1","executionInfo":{"status":"ok","timestamp":1627130182844,"user_tz":-540,"elapsed":240,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"ea8fd47d-82ac-4aae-b3d6-49801366b10d"},"source":["from sklearn.metrics import f1_score\n","\n","# 真の値 - [[1, 2], [1], [1, 2, 3], [2, 3], [3]]\n","y_true = np.array([[1, 1, 0], \n","          [1, 0, 0], \n","          [1, 1, 1], \n","          [0, 1, 1], \n","          [0, 0, 1]])\n","\n","# 予測値 - [[1, 3], [2], [1, 3], [3], [3]]\n","y_pred = np.array([[1, 0, 1], \n","          [0, 1, 0], \n","          [1, 0, 1], \n","          [0, 0, 1], \n","          [0, 0, 1]])\n","\n","mean_f1 = np.mean([f1_score(y_true[i, :], y_pred[i, :]) for i in range(len(y_true))])\n","macro_f1 = np.mean([f1_score(y_true[:, c], y_pred[:, c]) for c in range(y_true.shape[1])])\n","micro_f1 = f1_score(y_true.reshape(-1), y_pred.reshape(-1))\n","print(mean_f1, macro_f1, micro_f1)\n","\n","f1 = f1_score(y_true, y_pred, average=None)\n","mean_f1 = f1_score(y_true, y_pred, average='samples')\n","macro_f1 = f1_score(y_true, y_pred, average='macro')\n","micro_f1 = f1_score(y_true, y_pred, average='micro')\n","print(f1, mean_f1, macro_f1, micro_f1)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.5933333333333334 0.5523809523809523 0.6250000000000001\n","[0.8        0.         0.85714286] 0.5933333333333334 0.5523809523809523 0.6250000000000001\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mfTEECVz_Axw"},"source":["# **評価指標と目的関数**"]},{"cell_type":"markdown","metadata":{"id":"PPKhXMJS_Gy0"},"source":["## **評価指標と目的関数の違い**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"BT65UHwE_IN1"},"source":["- 目的関数：モデルの学習において最適化される関数.\n"," - モデルに応じて, 微分可能性などの制約が課される.\n"," - 回帰タスクでは RMSE, 分類タスクでは logloss が基本的.\n","\n","- 評価指標：モデルや予測値の性能の良し悪しを測る指標.\n"," - 真の値と予測値から計算できれば特に制約はない.\n"," - ビジネス上の価値判断などから決定する.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yqt11gnY-fyH"},"source":["# **評価指標の最適化**"]},{"cell_type":"markdown","metadata":{"id":"_xbD3FiW-1Si"},"source":["## **評価指標の最適化のアプローチ**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"5c_GdpdBErLN"},"source":["- 単に正しくモデリングを行う. \n"," - 評価指標と目的関数が同じ場合は, 単にモデルを学習・予測させることで評価指標に最適化される. \n","\n","- 学習データの前処理をして, 異なる評価指標を最適化する.\n"," - 例えば, 評価指標が RMSLE の場合に, 与えられた学習データの目的変数の対数をとって変換し, 目的関数を RMSE として学習させたあと, 指数関数で変換をもとに戻す方法が挙げられる.\n","\n","- 異なる評価指標の最適化を行い, 後処理を行う.\n"," - モデルを学習・予測させたあと, 評価指標の性質に基づいて計算したり, 最適化アルゴリズムを用いて閾値などを最適化する方法.\n","\n","- カスタム指標を使用する. \n","\n","- 異なる評価指標を最適化し, アーリーストッピングを行う.\n"," - アーリーストッピングの評価対象に最適化したい評価指標を設定し, その指標が最適になるような時点で学習を止める方法.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"s1g7pV-C-1eX"},"source":["## **閾値の最適化**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"nKW_3JGdLSV0"},"source":["- 最適化アルゴリズムを用いる方法\n"," - scipy.optimizeモジュールなどを用いて, 「閾値を引数にしてスコアを返す関数」を最適化する.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5qKudSdf7bZ","executionInfo":{"status":"ok","timestamp":1627177829467,"user_tz":-540,"elapsed":242,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"981b50b7-214a-413c-dfb8-088d846df6e1"},"source":["from sklearn.metrics import f1_score\n","from scipy.optimize import minimize\n","\n","rand = np.random.RandomState(seed=71)\n","train_y_prob = np.linspace(0, 1.0, 10000)\n","\n","train_y = pd.Series(rand.uniform(0.0, 1.0, train_y_prob.size) < train_y_prob)\n","train_pred_prob = np.clip(train_y_prob * np.exp(rand.standard_normal(train_y_prob.shape) * 0.3), 0.0, 1.0)\n","\n","init_threshold = 0.5\n","init_score = f1_score(train_y, train_pred_prob >= init_threshold)\n","print(init_threshold, init_score)\n","\n","def f1_opt(x):\n","  return -f1_score(train_y, train_pred_prob >= x)\n","\n","\n","result = minimize(f1_opt, x0=np.array([init_threshold]), method='Nelder-Mead')\n","best_threshold = result['x'].item()\n","best_score = f1_score(train_y, train_pred_prob >= best_threshold)\n","print(best_threshold, best_score)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.5 0.7224831529507862\n","0.32324218749999983 0.7557317703844165\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0kknl_4yXyd_"},"source":["## **out-of-fold による閾値の最適化**\n","\n","---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPP-37I3ObXz","executionInfo":{"status":"ok","timestamp":1627196228946,"user_tz":-540,"elapsed":618,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"08cce711-bbfd-4abf-f7dc-c5b91a6b5a23"},"source":["from scipy.optimize import minimize\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import KFold\n","\n","rand = np.random.RandomState(seed=71)\n","train_y_prob = np.linspace(0, 1.0, 10000)\n","\n","train_y = pd.Series(rand.uniform(0.0, 1.0, train_y_prob.size) < train_y_prob)\n","train_pred_prob = np.clip(train_y_prob * np.exp(rand.standard_normal(train_y_prob.shape) * 0.3), 0.0, 1.0)\n","\n","init_threshold = 0.5\n","\n","thresholds = []\n","scores_tr = []\n","scores_va = []\n","\n","kf = KFold(n_splits=4, random_state=71, shuffle=True)\n","for i, (tr_idx, va_idx) in enumerate(kf.split(train_pred_prob)):\n","  tr_pred_prob, va_pred_prob = train_pred_prob[tr_idx], train_pred_prob[va_idx]\n","  tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n","\n","  def f1_opt(x):\n","    return -f1_score(tr_y, tr_pred_prob >= x)\n","\n","\n","  result = minimize(f1_opt, x0=np.array([init_threshold]), method='Nelder-Mead')\n","  threshold = result['x'].item()\n","  score_tr = f1_score(tr_y, tr_pred_prob >= threshold)\n","  score_va = f1_score(va_y, va_pred_prob >= threshold)\n","  print(threshold, score_tr, score_va)\n","\n","  thresholds.append(threshold)\n","  scores_tr.append(score_tr)\n","  scores_va.append(score_va)\n","\n","threshold_test = np.mean(thresholds)\n","print(threshold_test)\n","  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.34257812499999984 0.7559183673469387 0.7570422535211268\n","0.34277343749999983 0.7598457403295548 0.7450980392156863\n","0.31787109374999983 0.7548253676470588 0.7584803256445047\n","0.3234374999999998 0.7545569184913447 0.7588603196664351\n","0.33166503906249983\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"08j2JCe9wsKc"},"source":["## **予測確率とその調整**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"1iKaM0zWwyDG"},"source":["### **予測確率が歪んでいる場合**\n","\n","- データが十分でない場合.\n"," - データが少ない時は, 特に極端に0や1に近い確率を予測することは難しい.\n","\n","- モデルの学習のアルゴリズム上, 妥当な確率を予測するように最適化されない場合"]},{"cell_type":"markdown","metadata":{"id":"8N_bAffDwyL7"},"source":["### **予測確率の調整**\n","\n","- 予測値をn乗する.\n"," - nは0.9～1.1程度として, 予測値をn乗する処理を最後に加えることがある. 確率を十分に予測できていないと考えて, 補正を試みていると言える.\n","\n","- 極端に0や1に近い確率のクリップ\n"," - 大きなペナルティを避けるためなどの理由で, 出力する確率の範囲を0.1%～99.9%等に制限する方法がある.\n","\n","- スタッキング"]},{"cell_type":"markdown","metadata":{"id":"JFIsqK4UJHhR"},"source":["# **欠損値の扱い**"]},{"cell_type":"markdown","metadata":{"id":"JPMv2FGkMZYX"},"source":["## **欠損値が発生する理由**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"7SWN4l54MsYm"},"source":["- 値が存在しないケース.\n"," - 個人と法人が混在しているデータでの法人の年齢, 人数が0の場合の平均など.\n","\n","- 何らかの意図があるケース.\n"," - 入力フォームにユーザが入力してくれない, その場所や時間については観測していないなど.\n","\n","- 値を取得するのに失敗したケース.\n"," - 人為的ミスや観測機器のエラーで値があるにも関わらず取得できなかったなど."]},{"cell_type":"markdown","metadata":{"id":"-oASl2vRJHx0"},"source":["## **欠損値のまま取り扱う**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"kZIn1voeX2Yr"},"source":["- GBDTでは欠損値を埋めずにそのまま取り扱うことができるため, そのまま取り扱うのが自然な方法."]},{"cell_type":"markdown","metadata":{"id":"pWh944IiX81G"},"source":["## **欠損値を代表値で埋める**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ECDrzXgMY22K"},"source":["- 欠損の発生がランダムであることを前提としている. ランダムでなければ, あまり適切でない可能性がある. \n","\n","- 代表値はデータの特性に応じて決定する. \n"," - 平均値. もっとも典型的.\n"," - 中央値. 分布が歪んでいる場合.\n"," - 変数変換後に平均値. 対数変換などにより歪みの少ない分布に変換.\n"," - 別のカテゴリ変数でグループ分けし1, 平均値. 欠損している変数の分布がグループごとに大きく変わることが想定される場合.\n"]},{"cell_type":"markdown","metadata":{"id":"ZDIoDJC8Y2_j"},"source":["## **欠損値を他の変数から予測する**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"nqhGar1dci2O"},"source":["1. 欠損を補完したい変数を目的変数とみなして, その他の変数を特徴量とした補完のためのモデルを作成し, 学習を行う. 補完したい変数が欠損していないレコードを学習データとし, 欠損しているデータを予測対象のデータとする.\n"," - 補完のためのモデルの特徴量に本来の目的変数を含めると, テストデータを補完できなくなるため, 含めない. 逆に, 本来のテストデータで補完したい変数が欠損していないレコードは, 補完のためのモデルの学習データとして利用できる.\n","\n","1. 補完のためのモデルで予測した値で欠損値を埋める.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aW5RJxx5cjBY"},"source":["## **欠損値から新たな特徴量を作成する**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"cAhBat7xJH9A"},"source":["- 欠損値の発生が完全にランダムに起こることはあまりなく, 発生に何らかの理由がある場合は欠損していること自体に情報があるため, 欠損値から特徴量を作成することが有効.\n","\n"," - 欠損値がある各変数に対して, 欠損しているか否かの二値変数を作成する.\n"," \n"," - レコードごとに欠損している変数の数をカウントする. \n","\n"," - 複数の変数の欠損の組み合わせを調べ, それらがいくつかのパターンに分類できるのであれば, どのパターンであるかを特徴量とする.\n"]},{"cell_type":"markdown","metadata":{"id":"w4w2wKTsnoOB"},"source":["# **数値変数の変換**"]},{"cell_type":"markdown","metadata":{"id":"A9kluhMhntAX"},"source":["## **標準化 (standardization)**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Iva73xncnzmL"},"source":["線形変換をして, 変数の平均を0, 標準偏差を1にする操作.\n","\n","\\begin{align*}\n","x^{\\prime} = \\frac{x - \\mu}{\\sigma}\n","\\end{align*}\n","\n","- 線形モデルにおいては, スケールが大きい変数ほどその回帰係数が小さくなり, 標準化を行わないとそのような変数に対して正則化がかかりにくくなってしまう.\n","\n","- ニューラルネットにおいては, 変数同士のスケールの差が大きいままでは学習が上手く進まないことが多い.\n","\n","- 変換の方法は以下のいずれか. \n"," - 学習データで平均と分散を計算し, それを用いて学習データ・テストデータを変換する. \n"," - 学習データとテストデータを結合して平均と分散を計算し, それを用いて学習データ・テストデータを変換する. \n","\n","- スパースな変数に対して変換を行った場合, スパース性が壊れることがあるため, 注意が必要.\n"]},{"cell_type":"code","metadata":{"id":"Tb8_N31wrcb6"},"source":["from sklearn.preprocessing import StandardScaler\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')\n","\n","num_cols = ['age', 'height', 'weight', 'amount',\n","            'medical_info_a1', 'medical_info_a2', 'medical_info_a3', 'medical_info_b1']\n","\n","# パターン1\n","scaler = StandardScaler()\n","scaler.fit(train_x[num_cols])\n","\n","train_x[num_cols] = scaler.transform(train_x[num_cols])\n","test_x[num_cols] = scaler.transform(test_x[num_cols])\n","\n","# パターン2\n","# scaler = StandardScaler()\n","# scaler.fit(pd.concat([train_x[num_cols], test_x[num_cols]]))\n","\n","# train_x[num_cols] = scaler.transform(train_x[num_cols])\n","# test_x[num_cols] = scaler.transform(test_x[num_cols])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xUZEoaewrVrD"},"source":["## **Min-Maxスケーリング**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"vuf_R4H0uWUN"},"source":["変数のとる範囲を0から1の区間に押し込める操作.\n","\n","\\begin{align*}\n","x^{\\prime} = \\frac{x - x_{min}}{x_{max} - x_{min}}\n","\\end{align*}\n","\n","- 変換後の平均がちょうど0にならない, 外れ値の影響をより受けやすいなどのデメリットがあるため, 標準化の方がよく使われる.\n","\n","- スパースな変数に対して変換を行った場合, スパース性が壊れることがあるため, 注意が必要.\n","\n"]},{"cell_type":"code","metadata":{"id":"asWqwI1HvdsU"},"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')\n","\n","num_cols = ['age', 'height', 'weight', 'amount',\n","            'medical_info_a1', 'medical_info_a2', 'medical_info_a3', 'medical_info_b1']\n","\n","scaler = MinMaxScaler()\n","scaler.fit(train_x[num_cols])\n","\n","train_x[num_cols] = scaler.transform(train_x[num_cols])\n","test_x[num_cols] = scaler.transform(test_x[num_cols])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QMOcXb-Yvd-v"},"source":["## **非線形変換**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"L1Rt7QRWyj1X"},"source":["### **対数変換**\n","\n","裾の重い分布に対して有効な変換.\n","\n","\\begin{align*}\n","& x_1 = \\log x \\\\\n","& x_2 = \\log (1 + x) \\\\\n","& x_3 = \\mathrm{sgn} \\, x \\ \\log |x| \n","\\end{align*}\n","\n","- 通常の対数変換.\n","- 0が値として含まれる場合には, 1を足してから対数変換. \n","- 負の値が含まれる場合には, 絶対値を対数変換した後に符号をかけて変換.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"8pvzd49f2OM1"},"source":["x = np.array([1.0, 10.0, 100.0, 1000.0, 10000.0])\n","\n","x1 = np.log(x)\n","\n","x2 = np.log1p(x)\n","\n","x3 = np.sign(x) * np.log(np.abs(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zreyvq9ZykBc"},"source":["### **Box-Cox変換**\n","\n","対数変換の一般化. 正の値のみを持つ変数に適用可能.\n","\n","\\begin{align*}\n","x^{\\prime} =\n","\\left \\{\n","\\begin{aligned}\n","& \\frac{x^{\\lambda} - 1}{\\lambda} \\qquad \\lambda \\neq 0  \\\\\n","& \\log x \\qquad \\lambda = 0\n","\\end{aligned}\n","\\right.\n","\\end{align*}\n"]},{"cell_type":"markdown","metadata":{"id":"sBG8P6lA6RYO"},"source":["### **Yeo-Johnson変換**\n","\n","対数変換の一般化. 負の値を持つ変数にも適用可能.\n","\n","\\begin{align*}\n","x^{\\prime} =\n","\\left \\{\n","\\begin{aligned}\n","& \\frac{(1 + x)^{\\lambda} - 1}{\\lambda} \\qquad \\lambda \\neq 0, x_i \\ge 0  \\\\\n","& \\log (1 + x) \\qquad \\lambda = 0, x_i \\ge 0 \\\\\n","& \\frac{-\\left((1-x)^{2-\\lambda} - 1\\right)}{2 - \\lambda} \\qquad \\lambda \\neq 2, x_i < 0 \\\\\n","& - \\log (1 - x) \\qquad \\lambda = 2, x_i < 0 \n","\\end{aligned}\n","\\right.\n","\\end{align*}\n"]},{"cell_type":"code","metadata":{"id":"nvol5zG08lrC"},"source":["from sklearn.preprocessing import PowerTransformer\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')\n","\n","num_cols = ['age', 'height', 'weight', 'amount',\n","            'medical_info_a1', 'medical_info_a2', 'medical_info_a3', 'medical_info_b1']\n","\n","pt = PowerTransformer(method='yeo-johnson')\n","pt.fit(train_x[num_cols])\n","\n","train_x = pt.transform(train_x[num_cols])\n","test_x = pt.transform(test_x[num_cols])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qrHdjGhiBF7y"},"source":["### **その他の非線形変換**\n","\n","- 絶対値をとる.\n","- 平方根をとる.\n","- べき乗をとる.\n","- 正の値かどうか, ゼロかどうかなどの二値変数とする.\n","- 数値の端数をとる. \n","- 四捨五入, 切り上げ, 切り捨てを行う.\n","\n","**#平方根変換がポアソン分布に対する分散安定化変換になっていることを記載すること**"]},{"cell_type":"markdown","metadata":{"id":"vSFvMQz4E1Ow"},"source":["## **clipping**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"dZ98TtpGFABh"},"source":["上限や下限を設定し, それを外れた値は上限や下限の値で置き換える操作.\n","\n","- 外れ値を排除することができる.\n","\n","- 適当に閾値を設定することもできるが, 分位点を閾値とすることで機械的に外れ値を置き換えることもできる."]},{"cell_type":"code","metadata":{"id":"8vTkauIbF10F"},"source":["train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')\n","\n","num_cols = ['age', 'height', 'weight', 'amount',\n","            'medical_info_a1', 'medical_info_a2', 'medical_info_a3', 'medical_info_b1']\n","\n","p01 = train_x[num_cols].quantile(0.01)\n","p99 = train_x[num_cols].quantile(0.99)\n","\n","train_x[num_cols] = train_x[num_cols].clip(p01, p99, axis=1)\n","test_x[num_cols] = test_x[num_cols].clip(p01, p99, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"snRLeV8yJ0YM"},"source":["## **binning**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ipICYYECJ0kl"},"source":["数値変数を区間ごとにグループ分けして, あえてカテゴリ変数として扱う方法.\n","\n","- 等間隔に分割する方法.\n","- 分位点を用いて分割する方法.\n","- 区間の区切りを明示的に指定して分割する方法."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-LD84l3uLZ1N","executionInfo":{"status":"ok","timestamp":1627298258166,"user_tz":-540,"elapsed":533,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"bf9d6289-aa9d-444c-8780-32c8931865e5"},"source":["x = np.array([1, 7, 5, 4, 6, 3])\n","\n","binned_1 = pd.cut(x, 3, labels=False)\n","print(binned_1)\n","\n","bin_edges_2 = [-float('inf'), np.quantile(x, 0.01), np.quantile(x, 0.99), float('inf')]\n","binned_2 = pd.cut(x, bin_edges_2, labels=False)\n","print(binned_2)\n","\n","bin_edges_3 = [-float('inf'), 3.0, 4.0, 5.0, 5.5, float('inf')]\n","binned_3 = pd.cut(x, bin_edges_3, labels=False)\n","print(binned_3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0 2 1 1 2 0]\n","[0 2 1 1 1 1]\n","[0 4 2 1 4 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0mski8f9i3uH"},"source":["## **順位への変換**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"u2Rt_Io8jTTb"},"source":["数値変数を大小関係に基づいた順位へと変換する方法.\n","\n","- 数値の大きさや感覚の情報をあえて捨てて, 大小関係のみを抽出する方法.\n"," - 単に順位に変換する.\n"," - 順位をレコード数で割り, 0から1の範囲にスケーリングする."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KD6ErYJUkM-k","executionInfo":{"status":"ok","timestamp":1627298912460,"user_tz":-540,"elapsed":359,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"88ee5c1a-9e55-4207-c080-b5d9a17a4cc9"},"source":["x = [10, 20, 30, 0, 40, 40]\n","\n","rank = pd.Series(x).rank()\n","print(rank.values)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[2.  3.  4.  1.  5.5 5.5]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kVn8k5YvkzPv"},"source":["## **RankGauss**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"TEtAjYVpk6to"},"source":["数値変数を順位に変換し, 0～1にスケーリングした後, 正規分布の分布関数の逆関数で変換することで, 正規分布に従う変数に変換する方法. "]},{"cell_type":"code","metadata":{"id":"_kk9YFM5mh0x"},"source":["from sklearn.preprocessing import QuantileTransformer\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')\n","\n","num_cols = ['age', 'height', 'weight', 'amount',\n","            'medical_info_a1', 'medical_info_a2', 'medical_info_a3', 'medical_info_b1']\n","\n","transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\n","transformer.fit(train_x[num_cols])\n","\n","train_x[num_cols] = transformer.transform(train_x[num_cols])\n","test_x[num_cols] = transformer.transform(test_x[num_cols])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLRRXBtN-cpw"},"source":["# **カテゴリ変数の変換**"]},{"cell_type":"markdown","metadata":{"id":"sGbTNx4x-gS6"},"source":["- カテゴリ変数は, 多くの機械学習モデルでそのまま分析に用いることができず, モデルごとに適した形への変換が必要. \n","\n","- 変数が文字列で表されているケースだけではなく, データ上は数値であっても値の大きさや順序に意味が無い場合には, カテゴリ変数として扱うべき.\n","\n","- テストデータにのみ存在するカテゴリがある場合, 何らかの考慮が必要. \n"," - 影響が微小であることを確認し, 特に対応しない.\n"," - 最頻値や予測によって補完する.\n"]},{"cell_type":"markdown","metadata":{"id":"GRIrqnY8-gfB"},"source":["## **one-hot encoding**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"YuFx8IhhFA0v"},"source":["カテゴリ変数に対する最も代表的なハンドリング方法で, カテゴリ変数の各水準に対して, その水準かどうかを表す0, 1の二値変数をそれぞれ作成する方法. \n","\n","- n個の水準を持つカテゴリ変数にone-hot encodingを適用すると, n個の二値変数の特徴量が作成される. これらの二値変数はダミー変数と呼ばれる.\n","\n","- one-hot encodingの重大な欠点は, 特徴量の数がカテゴリ変数の水準数に応じて増加する点. 特徴量が増えすぎると, 学習の計算時間や必要なメモリが大きく増えたり, モデルの性能に悪影響を与える. \n"," - one-hot encoding以外のencoding手法を検討する.\n"," - 何らかの規則でグルーピングして, カテゴリ変数の水準の数を減らす.\n"," - 頻度の少ないカテゴリをすべて「その他のカテゴリ」のようにまとめてしまう.\n","\n","- n-1個のダミー変数で十分であるが, 特に問題が生じないこと, 分析がしやすいことからn個のダミー変数を作成するのが一般的.\n"]},{"cell_type":"code","metadata":{"id":"N_yeHNh8BNm4"},"source":["train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')\n","\n","cat_cols = ['sex', 'product', 'medical_info_b2', 'medical_info_b3']\n","\n","all_x = pd.concat([train_x, test_x])\n","all_x = pd.get_dummies(all_x, columns=cat_cols)\n","\n","train_x = all_x.iloc[:train_x.shape[0], :].reset_index(drop=True)\n","test_x = all_x.iloc[train_x.shape[0]:, :].reset_index(drop=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1d54OOLOHnoL"},"source":["## **label encoding**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"IYqB11duK2MX"},"source":["カテゴリ変数の各水準を単純に整数に置き換える方法. \n","\n","- 通常, 水準を文字列として辞書順に並べた順のインデックスで置き換える.\n","\n","- 辞書順に並べたときのインデックスの数値は, ほとんどの場合本質的な意味を持たない. そのため, 決定木をベースにした手法以外では, label encodingによる特徴量を直接学習に用いるのは適切でない. \n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"qOL1O0OoMiU5"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')\n","\n","cat_cols = ['sex', 'product', 'medical_info_b2', 'medical_info_b3']\n","\n","for c in cat_cols:\n","  le = LabelEncoder()\n","  le.fit(train_x[c])\n","  train_x[c] = le.transform(train_x[c])\n","  test_x[c] = le.transform(test_x[c])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JXmeMUhceXUM"},"source":["## **frequency encoding**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"tBQ0tQ-pec04"},"source":["カテゴリ変数の各水準の出現回数もしくは出現頻度でカテゴリ変数を置き換える方法.\n","- label encodingの変形として, 辞書順に並べた順のインデックスでなく, 出現頻度順のインデックスで並べることができる. \n"," - 同率の値が発生することに注意."]},{"cell_type":"code","metadata":{"id":"1guOOm1pf-qo"},"source":["train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')\n","\n","cat_cols = ['sex', 'product', 'medical_info_b2', 'medical_info_b3']\n","\n","for c in cat_cols:\n","  freq = train_x[c].value_counts()\n","  train_x[c] = train_x[c].map(freq)\n","  test_x[c] = test_x[c].map(freq)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOti6jBKynZC"},"source":["## **target encoding**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"IrV1wr_jyxHW"},"source":["目的変数を用いてカテゴリ変数を数値に変換する方法.\n","\n","- 基本的には, カテゴリ変数の各水準における目的変数の平均値を学習データで集計し, その値で置換する.\n","\n","- GBDTなどの決定木ベースのモデルでは, label encodingよりtarget encodingの方が有効な場合が多い.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1SKajwEn1mJa"},"source":["### **target encodingの手法・実装**\n","\n","- 単純にデータ全体から平均をとってしまうと, 自身のレコードの目的変数をカテゴリ変数に取り込んでしまうため, リークしてしまう. \n","\n","- 学習データをtarget-encoding用のfoldに分割し, 各foldごとに自身のfold以外のデータで目的変数の平均値を計算することで, 自身のレコードの目的変数の値を含めずに変換を行うことができる.\n","\n","- target encoding用のfoldの数は, 4～10くらい.\n","\n","- テストデータに対しては, 学習データ全体の目的変数の平均値を計算して変換する.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Cmz_lUBw6JNc"},"source":["from sklearn.model_selection import KFold\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')\n","\n","cat_cols = ['sex', 'product', 'medical_info_b2', 'medical_info_b3']\n","\n","for c in cat_cols:\n","  data_tmp = pd.DataFrame({c:train_x[c], 'target': train_y})\n","  target_mean = data_tmp.groupby(c)['target'].mean()\n","  test_x[c] = test_x[c].map(target_mean)\n","\n","  tmp = np.repeat(np.nan, train_x.shape[0])\n","\n","  kf = KFold(n_splits=4, shuffle=True, random_state=72)\n","  for idx_1, idx_2 in kf.split(train_x):\n","    target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n","    tmp[idx_2] = train_x[c].iloc[idx_2].map(target_mean)\n","\n","  train_x[c] = tmp\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZvvAb4QAFFu"},"source":["### **target encodingの手法・実装～クロスバリデーションを行う場合～**\n","\n","- クロスバリデーションで上記と同じようにtarget encodingを行うためには, クロスバリデーションのfoldごとに変換をかけ直す必要がある. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIyx1NBYBVAo","executionInfo":{"status":"ok","timestamp":1627391136968,"user_tz":-540,"elapsed":606,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"8b7397e0-e6ca-4996-ee4a-81407c9ee869"},"source":["from sklearn.model_selection import KFold\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test.csv')\n","\n","cat_cols = ['sex', 'product', 'medical_info_b2', 'medical_info_b3']\n","\n","kf = KFold(n_splits=4, shuffle=True, random_state=71)\n","for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n","  tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n","  tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n","  for c in cat_cols:\n","    data_tmp = pd.DataFrame({c:tr_x[c], 'target': tr_y})\n","    target_mean = data_tmp.groupby(c)['target'].mean()\n","    va_x.loc[:, c] = va_x[c].map(target_mean)\n","\n","    tmp = np.repeat(np.nan, tr_x.shape[0])\n","\n","    kf_encoding = KFold(n_splits=4, shuffle=True, random_state=72)\n","    for idx_1, idx_2 in kf_encoding.split(tr_x):\n","      target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n","      tmp[idx_2] = tr_x[c].iloc[idx_2].map(target_mean)\n","\n","    tr_x.loc[:, c] = tmp"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  isetter(ilocs[0], value)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"riJ_WsQGE83B"},"source":["### **目的変数の平均の取り方**\n","\n","- 回帰の場合は, 目的変数の平均をとる. \n","- 二値分類の場合は, 正例のときは1, 負例のときは0として平均をとる.\n","- 多クラス分類の場合は, クラスの数だけ二値分類があると考えて, クラスの数だけtarget encodingによる特徴量を作る.\n","- 外れ値がある場合など, 目的変数の分布によっては, 平均値ではなく中央値などをとる.\n","- 評価指標がRMSLEであるなど, 対数をとって評価される場合は, 対数をとったうえで目的変数の平均を計算する."]},{"cell_type":"markdown","metadata":{"id":"Nt0HvALyIEDV"},"source":["### **順序変数の扱い**\n","\n","- 決定木系のモデルでは, 序列をそのまま整数に置き換えて数値変数として扱えば良い.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MJD0LwqNGLMs"},"source":["### **カテゴリ変数の値の意味を抽出する**\n","カテゴリ変数の水準が無意味な記号でなく, 何かの意味を持っている場合, 単にencodingをおこなってしまうとその情報が消えてしまうため, その意味を抽出する処理を行って特徴量を作成する.\n","\n","- ABC-00123やXYZ-00200のような型番の場合, 前半の英字3文字と後半の数字5文字に分割する.\n","\n","- 3, Eのように数字のものと英字のものが混じっている場合, 数字か否かを特徴量にする.\n","\n","- AB, ACE, BCDEのように文字数に違いがある場合, 文字数を特徴量にする."]},{"cell_type":"markdown","metadata":{"id":"KCZpU9yOLHv5"},"source":["# **変数の組み合わせ**"]},{"cell_type":"markdown","metadata":{"id":"F8HG4nSELH9J"},"source":["複数の変数を組み合わせることで, 変数同士の相互作用を表現する特徴量を作成できる. \n","\n","- データに関する背景知識を利用して, 組み合わせを考える.\n","\n","- モデルから出力される特徴量や相互作用の重要度を基にして, 組み合わせを考える."]},{"cell_type":"markdown","metadata":{"id":"SkkPJYoTMEdo"},"source":["### **数値変数×カテゴリ変数**\n","\n","- カテゴリ変数の水準ごとに, 数値変数の平均や分散といった統計量をとり, 新たな特徴量とする."]},{"cell_type":"markdown","metadata":{"id":"_b0KrC76MEop"},"source":["### **数値変数×数値変数**\n","\n","- 数値変数を加減乗除する, 余りをとる, 比較するといった方法で新たな特徴量を作成する. \n","\n","- GBDTは加法的なモデルであるため, 加減よりも乗除の特徴量を加えた方が有効であると考えられる."]},{"cell_type":"markdown","metadata":{"id":"jYa5s6-eMc8D"},"source":["### **カテゴリ変数×カテゴリ変数**\n","\n","- 複数のカテゴリ変数の組み合わせを新たなカテゴリ変数とする.\n"," - 文字列として変数同士を連結する.\n","\n","- カテゴリ変数同士の組み合わせで作成した変数の変換は, target encodingが有効.\n"," - 目的変数の平均を計算するグループがより細分化され, より特徴的な傾向をとらえることができる可能性が高まるため.\n"," - その分, 過学習のリスクは高まる.\n"]},{"cell_type":"markdown","metadata":{"id":"d6MHU6uDPQ-9"},"source":["### **行の統計量をとる**\n","\n","- 行方向, つまりレコードごとに複数の変数を対象として統計量をとり, 新たな特徴量とする. \n"," - 欠損値, ゼロ, 負の値の数をカウントする.\n"," - 平均, 分散, 最大, 最小などの統計量を計算する."]},{"cell_type":"markdown","metadata":{"id":"W74WvfyvVv9s"},"source":["# ***抽出***"]},{"cell_type":"markdown","metadata":{"id":"RC_spc6kV0el"},"source":["## **データ列指定による抽出**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"ZOsq4TpvXpUV"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","result = reserve_tb[[\"reserve_id\",\"hotel_id\",\"customer_id\",\"reserve_datetime\",\"checkin_date\",\"checkin_time\",\"checkout_date\"]]\n","\n","reserve_tb.drop([\"people_num\",\"total_price\"], axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZdHaHe8YWIwr"},"source":["## **条件指定による抽出**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"onC6Wz0lY6y_"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","date1 = \"2016-10-13\"\n","date2 = \"2016-10-14\"\n","result = reserve_tb.query('@date1 <= checkout_date <= @date2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D-Gb28gPWToB"},"source":["## **データ値に基づかないサンプリング**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"odpFSWMmZM4O"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","result_1 = reserve_tb.sample(frac=0.5)\n","\n","result_2 = reserve_tb.sample(n=100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ri9QW7beWYOJ"},"source":["## **集約IDに基づくサンプリング**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"E-qaOZEiaEDA"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","target = pd.Series(reserve_tb[\"customer_id\"].unique()).sample(frac=0.5)\n","result = reserve_tb[reserve_tb[\"customer_id\"].isin(target)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MmNJdW5IWldl"},"source":["# **集約**"]},{"cell_type":"markdown","metadata":{"id":"sfp1XIWBWun3"},"source":["## **データ数、種類数の算出**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"oeYRnYtqV3gY"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","result = reserve_tb.groupby('hotel_id').agg({'reserve_id': 'count', 'customer_id': 'nunique'})\n","result.reset_index(inplace=True)\n","result.columns = ['hotel_id', 'rsv_cnt', 'cus_cnt']\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WhfuCZlxWux8"},"source":["## **合計値の算出**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"_w3A9twJX7JK"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","result = reserve_tb.groupby(['hotel_id', 'people_num'])['total_price'].sum().reset_index()\n","result.rename(columns={'total_price': 'price_num'}, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uzigXJnDWu7N"},"source":["## **極値、代表値の算出**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"-qQ5GNenYqWq"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","result = reserve_tb \\\n","  .groupby('hotel_id') \\\n","  .agg({'total_price': ['max', 'min', 'mean', 'median', lambda x: np.percentile(x, q=20)]}) \\\n","  .reset_index()\n","result.columns = ['hotel_id', 'price_max', 'price_min', 'price_mean', 'price_median', 'price_20per']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V6P-j8HFWvGQ"},"source":["## **ばらつき具合の算出**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"5gL7WswqZjE2"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","result = reserve_tb \\\n","  .groupby('hotel_id') \\\n","  .agg({'total_price': ['var', 'std']}).reset_index()\n","result.columns = ['hotel_id', 'price_var', 'price_std']\n","result.fillna(value={'price_var': 0, 'price_std': 0}, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCKw5T1OWvRL"},"source":["## **最頻値の算出**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"PJoVUwDqbVaq"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","result = reserve_tb['total_price'].round(-3).mode()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vcLfbglAXKEP"},"source":["## **順位の算出**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"pWYzksL-b1X3"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","reserve_tb['reserve_datetime'] = pd.to_datetime(reserve_tb['reserve_datetime'], format='%Y-%m-%d %H:%M:%S')\n","reserve_tb['log_no'] = reserve_tb \\\n","  .groupby('customer_id')['reserve_datetime'] \\\n","  .rank(ascending=True, method='first')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ek6dRiIhdeo3"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","\n","rsv_cnt_tb = reserve_tb.groupby('hotel_id').size().reset_index()\n","rsv_cnt_tb.columns = ['hotel_id', 'rsv_cnt']\n","rsv_cnt_tb['rsv_cnt_rank'] = rsv_cnt_tb['rsv_cnt'] \\\n","        .rank(ascending=False, method='min')\n","rsv_cnt_tb.drop('rsv_cnt', axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KG9GdI7OiXPJ"},"source":["# **結合**"]},{"cell_type":"markdown","metadata":{"id":"UYErtuJziclU"},"source":["## **マスタテーブルの結合**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"wlkvfoZeinOP"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","hotel_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/hotel.csv')\n","\n","result = pd.merge(reserve_tb.query('people_num == 1'), hotel_tb.query('is_business'), on='hotel_id', how='inner')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lyHHG2jqilmV"},"source":["## **条件に応じた結合テーブルの切り替え**\n","\n","---"]},{"cell_type":"code","metadata":{"id":"xaHMdKfUinxE"},"source":["reserve_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/reserve.csv')\n","hotel_tb = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/hotel.csv')\n","\n","small_area_mst = hotel_tb \\\n","  .groupby(['big_area_name', 'small_area_name']) \\\n","  .size().reset_index()\n","small_area_mst.columns = ['big_area_name', 'small_area_name', 'hotel_cnt']\n","small_area_mst['join_area_id'] = \\\n","  np.where(small_area_mst['hotel_cnt'] - 1 >= 20, \n","    small_area_mst['small_area_name'], small_area_mst['big_area_name'])\n","small_area_mst.drop(['hotel_cnt', 'big_area_name'], axis=1, inplace=True)\n","base_hotel_mst = pd.merge(hotel_tb, small_area_mst, on='small_area_name') \\\n","  .loc[:, ['hotel_id', 'join_area_id']]\n","recommend_hotel_mst = pd.concat([\n","  hotel_tb[['small_area_name', 'hotel_id']].rename(columns={'small_area_name': 'join_area_id'}, inplace=False),\n","  hotel_tb[['big_area_name', 'hotel_id']].rename(columns={'big_area_name': 'join_area_id'}, inplace=False)\n","]\n",")\n","recommend_hotel_mst.rename(columns={'hotel_id': 'rec_hotel_id'}, inplace=True)\n","result = pd.merge(base_hotel_mst, recommend_hotel_mst, on='join_area_id') \\\n","  .loc[:, ['hotel_id', 'rec_hotel_id']] \\\n","    .query('hotel_id != rec_hotel_id')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQKPgnQ25Bqq"},"source":["# **モデルの作成**"]},{"cell_type":"markdown","metadata":{"id":"MGlqRkHB9K8K"},"source":["## **モデルに関連する用語とポイント**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"VK7sXtkJ9VU-"},"source":["### **過学習 (オーバーフィッティング)**\n","\n","- 学習データのランダムなノイズまで学習してしまい, 汎化性能が劣化すること.\n","\n","**※バイアス・バリアンス分解について記載すること**"]},{"cell_type":"markdown","metadata":{"id":"EcQ03DjEBPvm"},"source":["### **正則化 (regularization)**\n"," - モデルの目的関数に正則化項を付与し, モデルの過度な複雑化を防ぎ, 過学習を抑えること\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IN5M9j8DDivc"},"source":["### **アーリーストッピング**\n","\n","- 学習時にバリデーションデータのスコアをモニタリングし, 一定の間スコアが上がらない場合, 途中で学習を打ち切ること. \n","\n","- 過学習を防ぎ, 最適なイテレーション数を自動で求めるために利用する.\n","\n","- 本来, バリデーションで適切な評価を行うためには, 学習時にバリデーションデータの情報を使ってはいけないが, アーリーストッピングではイテレーション回数を決めるための参考として使ってしまっているため, フェアな評価よりバリデーションのスコアが良くなってしまう点に注意が必要."]},{"cell_type":"markdown","metadata":{"id":"eDPAAObsFplX"},"source":["### **バギング**\n","\n","- 同じ種類のモデルを並列に複数作成し, 組み合わせる. 各モデルの予測値の平均を取るなどして, 一つの予測値とする.\n","\n","- データや特徴量のランダムサンプリングを行う場合が多いが, 学習に用いる乱数シードを変えるだけのこともある.\n"]},{"cell_type":"markdown","metadata":{"id":"hcEVxgjkGBnb"},"source":["### **ブースティング**\n","\n","- 同じ種類のモデルを直列的に複数作成し, 組み合わせる. それまでの学習による予測値を補正しながら, 順に1つずつモデルを学習させる.\n"]},{"cell_type":"markdown","metadata":{"id":"Z0_VyPx1J6gG"},"source":["# **GBDT (勾配ブースティング木)**"]},{"cell_type":"markdown","metadata":{"id":"C39S9VFmJ6qm"},"source":["## **GBDTの特徴**\n","\n","---\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8O9KyyK00WDx"},"source":["- 特徴量は数値.\n"," - ある特徴量がある値より大きいか小さいかによって, 決定木の分岐で振り分けられるため, 特徴量は数値である必要がある.\n","\n","- 特徴量をスケーリングする必要がない.\n"," - 決定木ではそれぞれの特徴量について値の大小のみが問題となるため, スケーリングを行う必要がない.\n","\n","- カテゴリ変数の前処理において, one-hot encodingでなく, label encodingでよい.\n","\n","- 欠損値を扱うことができる.\n"," - 欠損値のときにも決定木の分岐でどちらかに振り分けられるため, 補完などの処理をせずに欠損値をそのまま扱うことができる.\n","\n"," - 変数間の相互作用が反映される. \n","  - 分岐の繰り返しによって, 変数間の相互作用が反映される.\n","\n","- 精度が高い.\n","\n","- パラメータチューニングをしなくても精度が出やすい.\n","\n","- 不要な特徴量を追加しても精度が落ちにくい."]},{"cell_type":"markdown","metadata":{"id":"ptmZ0msuWm3f"},"source":["## **xgboost**\n","\n","---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66EqofEzcHQK","executionInfo":{"status":"ok","timestamp":1627200489537,"user_tz":-540,"elapsed":2004,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"5139fe80-e572-4733-c5fb-1a70e4dfe990"},"source":["import xgboost as xgb\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import KFold\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train_preprocessed.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test_preprocessed.csv')\n","\n","kf = KFold(n_splits=4, shuffle=True, random_state=71)\n","tr_idx, va_idx = list(kf.split(train_x))[0]\n","tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n","tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n","\n","dtrain = xgb.DMatrix(tr_x, label=tr_y)\n","dvalid = xgb.DMatrix(va_x, label=va_y)\n","dtest = xgb.DMatrix(test_x)\n","\n","params = {'objective': 'reg:squarederror', 'silent': 1, 'random_state': 71, \n","          'eval_metric': ['rmse', 'mae']}\n","num_round = 500\n","\n","watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n","model = xgb.train(params, dtrain, num_round, evals=watchlist, early_stopping_rounds=20)\n","\n","va_pred = model.predict(dvalid, ntree_limit=model.best_ntree_limit)\n","score = np.sqrt(mean_squared_error(va_y, va_pred))\n","print(score)\n","\n","pred = model.predict(dtest, ntree_limit=model.best_ntree_limit)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0]\ttrain-rmse:0.415819\ttrain-mae:0.409378\teval-rmse:0.42246\teval-mae:0.415129\n","Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n","\n","Will train until eval-mae hasn't improved in 20 rounds.\n","[1]\ttrain-rmse:0.363124\ttrain-mae:0.344053\teval-rmse:0.377034\teval-mae:0.355165\n","[2]\ttrain-rmse:0.328561\ttrain-mae:0.295569\teval-rmse:0.347089\teval-mae:0.309185\n","[3]\ttrain-rmse:0.304118\ttrain-mae:0.258982\teval-rmse:0.329484\teval-mae:0.276763\n","[4]\ttrain-rmse:0.28739\ttrain-mae:0.231324\teval-rmse:0.318412\teval-mae:0.25268\n","[5]\ttrain-rmse:0.272308\ttrain-mae:0.208294\teval-rmse:0.310191\teval-mae:0.233516\n","[6]\ttrain-rmse:0.26212\ttrain-mae:0.191438\teval-rmse:0.306795\teval-mae:0.220609\n","[7]\ttrain-rmse:0.254324\ttrain-mae:0.178455\teval-rmse:0.303963\teval-mae:0.210765\n","[8]\ttrain-rmse:0.247447\ttrain-mae:0.168119\teval-rmse:0.302157\teval-mae:0.203558\n","[9]\ttrain-rmse:0.241156\ttrain-mae:0.160043\teval-rmse:0.299341\teval-mae:0.197842\n","[10]\ttrain-rmse:0.236214\ttrain-mae:0.154721\teval-rmse:0.29755\teval-mae:0.194564\n","[11]\ttrain-rmse:0.230466\ttrain-mae:0.149718\teval-rmse:0.295052\teval-mae:0.191349\n","[12]\ttrain-rmse:0.226876\ttrain-mae:0.146607\teval-rmse:0.293817\teval-mae:0.190448\n","[13]\ttrain-rmse:0.223752\ttrain-mae:0.143943\teval-rmse:0.293186\teval-mae:0.189475\n","[14]\ttrain-rmse:0.219679\ttrain-mae:0.141404\teval-rmse:0.292722\teval-mae:0.188777\n","[15]\ttrain-rmse:0.216461\ttrain-mae:0.138816\teval-rmse:0.291357\teval-mae:0.187625\n","[16]\ttrain-rmse:0.212159\ttrain-mae:0.135989\teval-rmse:0.290606\teval-mae:0.187153\n","[17]\ttrain-rmse:0.208557\ttrain-mae:0.133608\teval-rmse:0.289649\teval-mae:0.186172\n","[18]\ttrain-rmse:0.203184\ttrain-mae:0.131169\teval-rmse:0.287463\teval-mae:0.185701\n","[19]\ttrain-rmse:0.200416\ttrain-mae:0.129489\teval-rmse:0.287225\teval-mae:0.18554\n","[20]\ttrain-rmse:0.197387\ttrain-mae:0.127819\teval-rmse:0.28702\teval-mae:0.185478\n","[21]\ttrain-rmse:0.195849\ttrain-mae:0.126863\teval-rmse:0.28688\teval-mae:0.185759\n","[22]\ttrain-rmse:0.192196\ttrain-mae:0.124489\teval-rmse:0.286167\teval-mae:0.185062\n","[23]\ttrain-rmse:0.191033\ttrain-mae:0.123374\teval-rmse:0.286075\teval-mae:0.184793\n","[24]\ttrain-rmse:0.188382\ttrain-mae:0.121221\teval-rmse:0.285682\teval-mae:0.183841\n","[25]\ttrain-rmse:0.187073\ttrain-mae:0.120589\teval-rmse:0.285604\teval-mae:0.183939\n","[26]\ttrain-rmse:0.185862\ttrain-mae:0.119775\teval-rmse:0.285597\teval-mae:0.184051\n","[27]\ttrain-rmse:0.182351\ttrain-mae:0.117461\teval-rmse:0.284182\teval-mae:0.182964\n","[28]\ttrain-rmse:0.180242\ttrain-mae:0.115973\teval-rmse:0.283646\teval-mae:0.1826\n","[29]\ttrain-rmse:0.17795\ttrain-mae:0.114462\teval-rmse:0.283142\teval-mae:0.182487\n","[30]\ttrain-rmse:0.175794\ttrain-mae:0.1133\teval-rmse:0.282791\teval-mae:0.182382\n","[31]\ttrain-rmse:0.174896\ttrain-mae:0.112798\teval-rmse:0.282945\teval-mae:0.182672\n","[32]\ttrain-rmse:0.172826\ttrain-mae:0.111666\teval-rmse:0.283135\teval-mae:0.183045\n","[33]\ttrain-rmse:0.17135\ttrain-mae:0.110697\teval-rmse:0.283333\teval-mae:0.182895\n","[34]\ttrain-rmse:0.169213\ttrain-mae:0.109566\teval-rmse:0.283999\teval-mae:0.183384\n","[35]\ttrain-rmse:0.168808\ttrain-mae:0.109188\teval-rmse:0.284134\teval-mae:0.183378\n","[36]\ttrain-rmse:0.16725\ttrain-mae:0.108269\teval-rmse:0.28416\teval-mae:0.183371\n","[37]\ttrain-rmse:0.164286\ttrain-mae:0.106612\teval-rmse:0.284005\teval-mae:0.183221\n","[38]\ttrain-rmse:0.161268\ttrain-mae:0.105194\teval-rmse:0.283279\teval-mae:0.182777\n","[39]\ttrain-rmse:0.160024\ttrain-mae:0.104366\teval-rmse:0.283518\teval-mae:0.18296\n","[40]\ttrain-rmse:0.159301\ttrain-mae:0.103836\teval-rmse:0.283277\teval-mae:0.1829\n","[41]\ttrain-rmse:0.157547\ttrain-mae:0.102886\teval-rmse:0.283227\teval-mae:0.183148\n","[42]\ttrain-rmse:0.155139\ttrain-mae:0.101418\teval-rmse:0.28266\teval-mae:0.182987\n","[43]\ttrain-rmse:0.153161\ttrain-mae:0.10046\teval-rmse:0.282322\teval-mae:0.18295\n","[44]\ttrain-rmse:0.151333\ttrain-mae:0.099539\teval-rmse:0.281034\teval-mae:0.182367\n","[45]\ttrain-rmse:0.149438\ttrain-mae:0.098435\teval-rmse:0.280964\teval-mae:0.182485\n","[46]\ttrain-rmse:0.14883\ttrain-mae:0.097887\teval-rmse:0.280995\teval-mae:0.182541\n","[47]\ttrain-rmse:0.147947\ttrain-mae:0.097195\teval-rmse:0.281034\teval-mae:0.182511\n","[48]\ttrain-rmse:0.147357\ttrain-mae:0.096741\teval-rmse:0.280934\teval-mae:0.182484\n","[49]\ttrain-rmse:0.14536\ttrain-mae:0.095688\teval-rmse:0.280652\teval-mae:0.182424\n","[50]\ttrain-rmse:0.143368\ttrain-mae:0.094497\teval-rmse:0.280648\teval-mae:0.1824\n","[51]\ttrain-rmse:0.142538\ttrain-mae:0.093941\teval-rmse:0.28072\teval-mae:0.182454\n","[52]\ttrain-rmse:0.141151\ttrain-mae:0.093169\teval-rmse:0.281107\teval-mae:0.182915\n","[53]\ttrain-rmse:0.139357\ttrain-mae:0.092282\teval-rmse:0.280828\teval-mae:0.182795\n","[54]\ttrain-rmse:0.13912\ttrain-mae:0.092041\teval-rmse:0.280897\teval-mae:0.182816\n","[55]\ttrain-rmse:0.138504\ttrain-mae:0.091573\teval-rmse:0.280981\teval-mae:0.182797\n","[56]\ttrain-rmse:0.136455\ttrain-mae:0.090367\teval-rmse:0.281314\teval-mae:0.182962\n","[57]\ttrain-rmse:0.134567\ttrain-mae:0.089306\teval-rmse:0.281558\teval-mae:0.183335\n","[58]\ttrain-rmse:0.13285\ttrain-mae:0.088161\teval-rmse:0.281603\teval-mae:0.183276\n","[59]\ttrain-rmse:0.13183\ttrain-mae:0.08754\teval-rmse:0.281813\teval-mae:0.183428\n","[60]\ttrain-rmse:0.130121\ttrain-mae:0.086729\teval-rmse:0.281945\teval-mae:0.183544\n","[61]\ttrain-rmse:0.129788\ttrain-mae:0.086451\teval-rmse:0.281862\teval-mae:0.183503\n","[62]\ttrain-rmse:0.129647\ttrain-mae:0.086316\teval-rmse:0.281918\teval-mae:0.183515\n","[63]\ttrain-rmse:0.128857\ttrain-mae:0.085831\teval-rmse:0.281975\teval-mae:0.183645\n","[64]\ttrain-rmse:0.127796\ttrain-mae:0.085081\teval-rmse:0.281937\teval-mae:0.183629\n","Stopping. Best iteration:\n","[44]\ttrain-rmse:0.151333\ttrain-mae:0.099539\teval-rmse:0.281034\teval-mae:0.182367\n","\n","0.2810343406054346\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kf7Ih7DiWn1k"},"source":["## **lightgbm**\n","\n","---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfXGz4HhhWKp","executionInfo":{"status":"ok","timestamp":1627200668149,"user_tz":-540,"elapsed":797,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"b09a7bcc-250c-4aec-ffe0-4751197516b9"},"source":["import lightgbm as lgb\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import KFold\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train_preprocessed.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test_preprocessed.csv')\n","\n","kf = KFold(n_splits=4, shuffle=True, random_state=71)\n","tr_idx, va_idx = list(kf.split(train_x))[0]\n","tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n","tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n","\n","lgb_train = lgb.Dataset(tr_x, tr_y)\n","lgb_eval = lgb.Dataset(va_x, va_y)\n","\n","params = {'objective': 'regression', 'seed': 71, 'verbose': 0, 'metrics': ['l1', 'l2']}\n","num_round = 500\n","\n","model = lgb.train(params, lgb_train, num_boost_round=num_round, \n","                  valid_names=['train', 'valid'], valid_sets=[lgb_train, lgb_eval], \n","                  early_stopping_rounds=20)\n","\n","va_pred = model.predict(va_x, num_iteration=model.best_iteration)\n","score = np.sqrt(mean_squared_error(va_y, va_pred))\n","print(score)\n","\n","pred = model.predict(test_x, num_iteration=model.best_iteration)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1]\ttrain's l2: 0.144155\ttrain's l1: 0.298012\tvalid's l2: 0.148649\tvalid's l1: 0.302593\n","Training until validation scores don't improve for 20 rounds.\n","[2]\ttrain's l2: 0.135615\ttrain's l1: 0.288178\tvalid's l2: 0.140439\tvalid's l1: 0.293427\n","[3]\ttrain's l2: 0.128532\ttrain's l1: 0.279298\tvalid's l2: 0.13465\tvalid's l1: 0.28589\n","[4]\ttrain's l2: 0.122351\ttrain's l1: 0.271028\tvalid's l2: 0.128931\tvalid's l1: 0.278559\n","[5]\ttrain's l2: 0.11703\ttrain's l1: 0.2633\tvalid's l2: 0.124222\tvalid's l1: 0.271601\n","[6]\ttrain's l2: 0.112223\ttrain's l1: 0.255997\tvalid's l2: 0.120011\tvalid's l1: 0.264942\n","[7]\ttrain's l2: 0.107873\ttrain's l1: 0.24922\tvalid's l2: 0.116515\tvalid's l1: 0.259252\n","[8]\ttrain's l2: 0.104128\ttrain's l1: 0.242916\tvalid's l2: 0.113325\tvalid's l1: 0.253602\n","[9]\ttrain's l2: 0.100812\ttrain's l1: 0.237399\tvalid's l2: 0.110685\tvalid's l1: 0.249135\n","[10]\ttrain's l2: 0.0976968\ttrain's l1: 0.232159\tvalid's l2: 0.108185\tvalid's l1: 0.244438\n","[11]\ttrain's l2: 0.0951578\ttrain's l1: 0.227392\tvalid's l2: 0.106005\tvalid's l1: 0.240145\n","[12]\ttrain's l2: 0.0922927\ttrain's l1: 0.222366\tvalid's l2: 0.103771\tvalid's l1: 0.235978\n","[13]\ttrain's l2: 0.089938\ttrain's l1: 0.218193\tvalid's l2: 0.102008\tvalid's l1: 0.232438\n","[14]\ttrain's l2: 0.0877061\ttrain's l1: 0.213899\tvalid's l2: 0.100368\tvalid's l1: 0.228882\n","[15]\ttrain's l2: 0.0855368\ttrain's l1: 0.209828\tvalid's l2: 0.0984655\tvalid's l1: 0.225218\n","[16]\ttrain's l2: 0.0836673\ttrain's l1: 0.20588\tvalid's l2: 0.0971296\tvalid's l1: 0.221922\n","[17]\ttrain's l2: 0.0818074\ttrain's l1: 0.202011\tvalid's l2: 0.0959439\tvalid's l1: 0.218892\n","[18]\ttrain's l2: 0.0800678\ttrain's l1: 0.198619\tvalid's l2: 0.0947737\tvalid's l1: 0.216236\n","[19]\ttrain's l2: 0.0780923\ttrain's l1: 0.194732\tvalid's l2: 0.093101\tvalid's l1: 0.212786\n","[20]\ttrain's l2: 0.0767093\ttrain's l1: 0.192109\tvalid's l2: 0.0921485\tvalid's l1: 0.210652\n","[21]\ttrain's l2: 0.0752543\ttrain's l1: 0.188958\tvalid's l2: 0.0911491\tvalid's l1: 0.208044\n","[22]\ttrain's l2: 0.0737669\ttrain's l1: 0.185767\tvalid's l2: 0.09029\tvalid's l1: 0.205553\n","[23]\ttrain's l2: 0.0725938\ttrain's l1: 0.183264\tvalid's l2: 0.0898926\tvalid's l1: 0.204134\n","[24]\ttrain's l2: 0.0711864\ttrain's l1: 0.180268\tvalid's l2: 0.0887358\tvalid's l1: 0.2014\n","[25]\ttrain's l2: 0.0696558\ttrain's l1: 0.177289\tvalid's l2: 0.0874847\tvalid's l1: 0.198827\n","[26]\ttrain's l2: 0.0684513\ttrain's l1: 0.174503\tvalid's l2: 0.0867325\tvalid's l1: 0.196591\n","[27]\ttrain's l2: 0.0675547\ttrain's l1: 0.172375\tvalid's l2: 0.0861833\tvalid's l1: 0.19512\n","[28]\ttrain's l2: 0.0666711\ttrain's l1: 0.170478\tvalid's l2: 0.0857267\tvalid's l1: 0.193754\n","[29]\ttrain's l2: 0.065846\ttrain's l1: 0.168549\tvalid's l2: 0.0850272\tvalid's l1: 0.192134\n","[30]\ttrain's l2: 0.0649932\ttrain's l1: 0.166947\tvalid's l2: 0.0847253\tvalid's l1: 0.191336\n","[31]\ttrain's l2: 0.0641442\ttrain's l1: 0.16503\tvalid's l2: 0.0842798\tvalid's l1: 0.190006\n","[32]\ttrain's l2: 0.0633529\ttrain's l1: 0.163584\tvalid's l2: 0.0835516\tvalid's l1: 0.188765\n","[33]\ttrain's l2: 0.0626441\ttrain's l1: 0.162362\tvalid's l2: 0.0832135\tvalid's l1: 0.188109\n","[34]\ttrain's l2: 0.0619018\ttrain's l1: 0.161066\tvalid's l2: 0.0829313\tvalid's l1: 0.187346\n","[35]\ttrain's l2: 0.0613125\ttrain's l1: 0.160069\tvalid's l2: 0.0826736\tvalid's l1: 0.1869\n","[36]\ttrain's l2: 0.060439\ttrain's l1: 0.158709\tvalid's l2: 0.0822064\tvalid's l1: 0.186037\n","[37]\ttrain's l2: 0.0595297\ttrain's l1: 0.157317\tvalid's l2: 0.081548\tvalid's l1: 0.185121\n","[38]\ttrain's l2: 0.0589548\ttrain's l1: 0.156265\tvalid's l2: 0.081329\tvalid's l1: 0.184548\n","[39]\ttrain's l2: 0.0583505\ttrain's l1: 0.155256\tvalid's l2: 0.0811834\tvalid's l1: 0.184137\n","[40]\ttrain's l2: 0.0573877\ttrain's l1: 0.153871\tvalid's l2: 0.0806919\tvalid's l1: 0.183456\n","[41]\ttrain's l2: 0.0569203\ttrain's l1: 0.153088\tvalid's l2: 0.0805121\tvalid's l1: 0.183147\n","[42]\ttrain's l2: 0.0563825\ttrain's l1: 0.152192\tvalid's l2: 0.0803245\tvalid's l1: 0.182746\n","[43]\ttrain's l2: 0.0558333\ttrain's l1: 0.151423\tvalid's l2: 0.0801188\tvalid's l1: 0.182443\n","[44]\ttrain's l2: 0.0553239\ttrain's l1: 0.150706\tvalid's l2: 0.0800942\tvalid's l1: 0.18228\n","[45]\ttrain's l2: 0.0543943\ttrain's l1: 0.149631\tvalid's l2: 0.0794295\tvalid's l1: 0.181541\n","[46]\ttrain's l2: 0.0537967\ttrain's l1: 0.148924\tvalid's l2: 0.0792595\tvalid's l1: 0.181369\n","[47]\ttrain's l2: 0.0533554\ttrain's l1: 0.148321\tvalid's l2: 0.0791028\tvalid's l1: 0.181243\n","[48]\ttrain's l2: 0.0527926\ttrain's l1: 0.147571\tvalid's l2: 0.0788605\tvalid's l1: 0.180883\n","[49]\ttrain's l2: 0.0519828\ttrain's l1: 0.146596\tvalid's l2: 0.078084\tvalid's l1: 0.180129\n","[50]\ttrain's l2: 0.0515343\ttrain's l1: 0.145994\tvalid's l2: 0.0779732\tvalid's l1: 0.180004\n","[51]\ttrain's l2: 0.0510485\ttrain's l1: 0.145376\tvalid's l2: 0.0778113\tvalid's l1: 0.179825\n","[52]\ttrain's l2: 0.0505737\ttrain's l1: 0.144785\tvalid's l2: 0.0776538\tvalid's l1: 0.179629\n","[53]\ttrain's l2: 0.050139\ttrain's l1: 0.144258\tvalid's l2: 0.0774341\tvalid's l1: 0.179389\n","[54]\ttrain's l2: 0.0494281\ttrain's l1: 0.143488\tvalid's l2: 0.0769175\tvalid's l1: 0.178973\n","[55]\ttrain's l2: 0.049067\ttrain's l1: 0.142947\tvalid's l2: 0.0767837\tvalid's l1: 0.178861\n","[56]\ttrain's l2: 0.0486734\ttrain's l1: 0.142561\tvalid's l2: 0.0766092\tvalid's l1: 0.178809\n","[57]\ttrain's l2: 0.0483079\ttrain's l1: 0.142134\tvalid's l2: 0.0764908\tvalid's l1: 0.178737\n","[58]\ttrain's l2: 0.0479572\ttrain's l1: 0.141677\tvalid's l2: 0.0763563\tvalid's l1: 0.178613\n","[59]\ttrain's l2: 0.0474345\ttrain's l1: 0.140988\tvalid's l2: 0.0760794\tvalid's l1: 0.178256\n","[60]\ttrain's l2: 0.0470732\ttrain's l1: 0.140472\tvalid's l2: 0.0759163\tvalid's l1: 0.178078\n","[61]\ttrain's l2: 0.0467441\ttrain's l1: 0.140066\tvalid's l2: 0.0758672\tvalid's l1: 0.178036\n","[62]\ttrain's l2: 0.0463739\ttrain's l1: 0.139528\tvalid's l2: 0.0757409\tvalid's l1: 0.177938\n","[63]\ttrain's l2: 0.0460847\ttrain's l1: 0.139147\tvalid's l2: 0.075776\tvalid's l1: 0.178055\n","[64]\ttrain's l2: 0.0457548\ttrain's l1: 0.138818\tvalid's l2: 0.0756389\tvalid's l1: 0.178128\n","[65]\ttrain's l2: 0.0452301\ttrain's l1: 0.138287\tvalid's l2: 0.0753335\tvalid's l1: 0.177949\n","[66]\ttrain's l2: 0.0447121\ttrain's l1: 0.137711\tvalid's l2: 0.0749131\tvalid's l1: 0.177578\n","[67]\ttrain's l2: 0.044388\ttrain's l1: 0.137223\tvalid's l2: 0.074994\tvalid's l1: 0.177735\n","[68]\ttrain's l2: 0.0440603\ttrain's l1: 0.136925\tvalid's l2: 0.0749292\tvalid's l1: 0.177878\n","[69]\ttrain's l2: 0.0436982\ttrain's l1: 0.136484\tvalid's l2: 0.0748201\tvalid's l1: 0.177763\n","[70]\ttrain's l2: 0.0434051\ttrain's l1: 0.136\tvalid's l2: 0.0747337\tvalid's l1: 0.177679\n","[71]\ttrain's l2: 0.0431336\ttrain's l1: 0.135741\tvalid's l2: 0.0747168\tvalid's l1: 0.177774\n","[72]\ttrain's l2: 0.0428429\ttrain's l1: 0.135459\tvalid's l2: 0.0746852\tvalid's l1: 0.177947\n","[73]\ttrain's l2: 0.0426022\ttrain's l1: 0.135091\tvalid's l2: 0.0745861\tvalid's l1: 0.177878\n","[74]\ttrain's l2: 0.0422857\ttrain's l1: 0.134604\tvalid's l2: 0.0744206\tvalid's l1: 0.177609\n","[75]\ttrain's l2: 0.0420411\ttrain's l1: 0.134259\tvalid's l2: 0.0743493\tvalid's l1: 0.177538\n","[76]\ttrain's l2: 0.0417771\ttrain's l1: 0.134002\tvalid's l2: 0.0743501\tvalid's l1: 0.177651\n","[77]\ttrain's l2: 0.0414756\ttrain's l1: 0.133509\tvalid's l2: 0.0742822\tvalid's l1: 0.17756\n","[78]\ttrain's l2: 0.0412342\ttrain's l1: 0.133169\tvalid's l2: 0.0742142\tvalid's l1: 0.177535\n","[79]\ttrain's l2: 0.0409243\ttrain's l1: 0.132683\tvalid's l2: 0.0741717\tvalid's l1: 0.177524\n","[80]\ttrain's l2: 0.0406669\ttrain's l1: 0.132308\tvalid's l2: 0.0741654\tvalid's l1: 0.177544\n","[81]\ttrain's l2: 0.0403946\ttrain's l1: 0.131997\tvalid's l2: 0.0741373\tvalid's l1: 0.177605\n","[82]\ttrain's l2: 0.0401889\ttrain's l1: 0.131635\tvalid's l2: 0.0741473\tvalid's l1: 0.177572\n","[83]\ttrain's l2: 0.0399932\ttrain's l1: 0.131312\tvalid's l2: 0.0741387\tvalid's l1: 0.17755\n","[84]\ttrain's l2: 0.0397709\ttrain's l1: 0.131053\tvalid's l2: 0.0741959\tvalid's l1: 0.177675\n","[85]\ttrain's l2: 0.0395659\ttrain's l1: 0.13076\tvalid's l2: 0.07419\tvalid's l1: 0.17769\n","[86]\ttrain's l2: 0.0393468\ttrain's l1: 0.130419\tvalid's l2: 0.0741218\tvalid's l1: 0.177618\n","[87]\ttrain's l2: 0.0391648\ttrain's l1: 0.130156\tvalid's l2: 0.0741636\tvalid's l1: 0.177727\n","[88]\ttrain's l2: 0.038967\ttrain's l1: 0.129845\tvalid's l2: 0.0741902\tvalid's l1: 0.177732\n","[89]\ttrain's l2: 0.0387583\ttrain's l1: 0.129559\tvalid's l2: 0.0741752\tvalid's l1: 0.177794\n","[90]\ttrain's l2: 0.0385379\ttrain's l1: 0.129231\tvalid's l2: 0.0741604\tvalid's l1: 0.177782\n","[91]\ttrain's l2: 0.0380732\ttrain's l1: 0.128767\tvalid's l2: 0.0738326\tvalid's l1: 0.177618\n","[92]\ttrain's l2: 0.0378843\ttrain's l1: 0.128457\tvalid's l2: 0.0738739\tvalid's l1: 0.177653\n","[93]\ttrain's l2: 0.0376258\ttrain's l1: 0.128105\tvalid's l2: 0.0737852\tvalid's l1: 0.177614\n","[94]\ttrain's l2: 0.0374501\ttrain's l1: 0.127871\tvalid's l2: 0.073807\tvalid's l1: 0.177698\n","[95]\ttrain's l2: 0.0372537\ttrain's l1: 0.127581\tvalid's l2: 0.0738131\tvalid's l1: 0.177807\n","[96]\ttrain's l2: 0.037066\ttrain's l1: 0.127267\tvalid's l2: 0.0738737\tvalid's l1: 0.177936\n","[97]\ttrain's l2: 0.036892\ttrain's l1: 0.126978\tvalid's l2: 0.0738383\tvalid's l1: 0.177974\n","[98]\ttrain's l2: 0.036746\ttrain's l1: 0.126738\tvalid's l2: 0.0737842\tvalid's l1: 0.177922\n","[99]\ttrain's l2: 0.0365539\ttrain's l1: 0.126513\tvalid's l2: 0.0737385\tvalid's l1: 0.177961\n","Early stopping, best iteration is:\n","[79]\ttrain's l2: 0.0409243\ttrain's l1: 0.132683\tvalid's l2: 0.0741717\tvalid's l1: 0.177524\n","0.2723448906735551\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MGw5a28DyZ0L"},"source":["# **モデルの評価**"]},{"cell_type":"markdown","metadata":{"id":"iQKDtAiR16w0"},"source":["- 予測モデルを作成する主な目的は, 未知のデータに対して高い精度で予測を行うこと.\n","\n","- モデルの汎化性能を改善していくためには, 汎化性能を適切に評価することが必要. \n","\n","- モデルの汎化性能を評価することをバリデーションと呼ぶ.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4pm7AkiT4s-R"},"source":["# **バリデーションの手法**"]},{"cell_type":"markdown","metadata":{"id":"lVPYrFm44tKE"},"source":["## **hold-out法**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"y4JHfJ9z4tYM"},"source":["- 学習データを学習用データとバリデーションデータに分割した後, 学習用データでモデルを学習し, バリデーションデータでモデルを評価する.\n","\n","- クロスバリデーションと比較すると, データを有効に使えていない欠点がある. \n","\n","- 最終的に学習データ全体でモデルを作成し直すことができるが, データ数が違うと最適なハイパーパラメータや特徴量が変わってくることもあるため, バリデーションにおいても学習用データはある程度確保することが望ましい.\n","\n","- データはシャッフルして用いる."]},{"cell_type":"code","metadata":{"id":"4qbNVEYX88HQ"},"source":["from sklearn.model_selection import KFold\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train_preprocessed.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test_preprocessed.csv')\n","\n","kf = KFold(n_splits=4, shuffle=True, random_state=71)\n","tr_idx, va_idx = list(kf.split(train_x))[0]\n","tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n","tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQ80smse83Kf"},"source":["## **クロスバリデーション**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"EXIV3_dq83TV"},"source":["- 学習データを分割し, hold-out法の手続きを複数回繰り返す手法. \n","\n","- 分割されたデータをfoldと呼び, 分割数をfold数と呼ぶ.\n","\n","- fold数を増やすほど学習用データの量を確保でき, データ全体で学習させた場合に近い精度評価ができる. 一方, fold数を増やすほど計算時間が増える.\n","\n","- fold数は4もしくは5とする場合が多い.\n","\n","- モデルの汎化性能を評価する際は, 通常は各foldにおけるスコアを平均して行う. 場合によっては, それぞれのfoldの目的変数と予測値を集めてデータ全体でスコアを算出する.\n","\n","- 最終的な予測モデルの作成方法は以下の2つに分けられる. \n"," - 各foldで学習したモデルを保存しておき, それらのモデルの予測値の平均などをとる.\n"," - 同様のハイパーパラメータで, 学習データ全体に対して改めてモデルを学習させて, 予測モデルを作成する.\n","\n","**※情報量基準との関係など詳細を記載すること**\n"]},{"cell_type":"code","metadata":{"id":"CcZv422At_Ow","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627224859880,"user_tz":-540,"elapsed":4352,"user":{"displayName":"Kenta Itasaka","photoUrl":"","userId":"09712003102127784369"}},"outputId":"e57ca48f-328c-4be4-934b-196027e19190"},"source":["import lightgbm as lgb\n","from sklearn.metrics import log_loss\n","from sklearn.model_selection import KFold\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train_preprocessed.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test_preprocessed.csv')\n","\n","scores = []\n","\n","kf = KFold(n_splits=4, shuffle=True, random_state=71)\n","for tr_idx, va_idx in kf.split(train_x):\n","  tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n","  tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n","\n","  lgb_train = lgb.Dataset(tr_x, tr_y)\n","  lgb_eval = lgb.Dataset(va_x, va_y)\n","\n","  params = {'objective': 'binary', 'seed': 71, 'verbose': 0, 'metrics': 'binary_logloss'}\n","  num_round = 500\n","\n","  model = lgb.train(params, lgb_train, num_boost_round=num_round, \n","                    valid_names=['train', 'valid'], valid_sets=[lgb_train, lgb_eval], \n","                    early_stopping_rounds=20)\n","\n","  va_pred = model.predict(va_x)\n","  score = log_loss(va_y, va_pred)\n","  scores.append(score)\n","\n","print(np.mean(score))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1]\ttrain's binary_logloss: 0.454308\tvalid's binary_logloss: 0.465515\n","Training until validation scores don't improve for 20 rounds.\n","[2]\ttrain's binary_logloss: 0.429565\tvalid's binary_logloss: 0.443444\n","[3]\ttrain's binary_logloss: 0.410077\tvalid's binary_logloss: 0.425543\n","[4]\ttrain's binary_logloss: 0.39358\tvalid's binary_logloss: 0.410625\n","[5]\ttrain's binary_logloss: 0.379354\tvalid's binary_logloss: 0.397666\n","[6]\ttrain's binary_logloss: 0.365913\tvalid's binary_logloss: 0.387422\n","[7]\ttrain's binary_logloss: 0.354309\tvalid's binary_logloss: 0.376037\n","[8]\ttrain's binary_logloss: 0.344354\tvalid's binary_logloss: 0.366734\n","[9]\ttrain's binary_logloss: 0.334834\tvalid's binary_logloss: 0.35898\n","[10]\ttrain's binary_logloss: 0.326209\tvalid's binary_logloss: 0.351612\n","[11]\ttrain's binary_logloss: 0.317809\tvalid's binary_logloss: 0.34563\n","[12]\ttrain's binary_logloss: 0.310845\tvalid's binary_logloss: 0.340564\n","[13]\ttrain's binary_logloss: 0.30401\tvalid's binary_logloss: 0.334274\n","[14]\ttrain's binary_logloss: 0.296333\tvalid's binary_logloss: 0.327911\n","[15]\ttrain's binary_logloss: 0.290137\tvalid's binary_logloss: 0.324239\n","[16]\ttrain's binary_logloss: 0.283293\tvalid's binary_logloss: 0.317865\n","[17]\ttrain's binary_logloss: 0.277823\tvalid's binary_logloss: 0.314412\n","[18]\ttrain's binary_logloss: 0.272322\tvalid's binary_logloss: 0.310773\n","[19]\ttrain's binary_logloss: 0.26705\tvalid's binary_logloss: 0.307374\n","[20]\ttrain's binary_logloss: 0.262091\tvalid's binary_logloss: 0.303946\n","[21]\ttrain's binary_logloss: 0.257394\tvalid's binary_logloss: 0.301154\n","[22]\ttrain's binary_logloss: 0.253115\tvalid's binary_logloss: 0.298085\n","[23]\ttrain's binary_logloss: 0.24878\tvalid's binary_logloss: 0.294533\n","[24]\ttrain's binary_logloss: 0.243951\tvalid's binary_logloss: 0.291832\n","[25]\ttrain's binary_logloss: 0.240469\tvalid's binary_logloss: 0.289538\n","[26]\ttrain's binary_logloss: 0.236038\tvalid's binary_logloss: 0.285777\n","[27]\ttrain's binary_logloss: 0.231149\tvalid's binary_logloss: 0.281554\n","[28]\ttrain's binary_logloss: 0.227936\tvalid's binary_logloss: 0.279469\n","[29]\ttrain's binary_logloss: 0.224501\tvalid's binary_logloss: 0.278255\n","[30]\ttrain's binary_logloss: 0.221476\tvalid's binary_logloss: 0.27687\n","[31]\ttrain's binary_logloss: 0.218174\tvalid's binary_logloss: 0.274434\n","[32]\ttrain's binary_logloss: 0.214462\tvalid's binary_logloss: 0.272274\n","[33]\ttrain's binary_logloss: 0.21088\tvalid's binary_logloss: 0.269122\n","[34]\ttrain's binary_logloss: 0.207684\tvalid's binary_logloss: 0.266756\n","[35]\ttrain's binary_logloss: 0.205168\tvalid's binary_logloss: 0.265291\n","[36]\ttrain's binary_logloss: 0.202792\tvalid's binary_logloss: 0.264635\n","[37]\ttrain's binary_logloss: 0.200275\tvalid's binary_logloss: 0.263032\n","[38]\ttrain's binary_logloss: 0.19745\tvalid's binary_logloss: 0.261202\n","[39]\ttrain's binary_logloss: 0.194507\tvalid's binary_logloss: 0.25904\n","[40]\ttrain's binary_logloss: 0.192425\tvalid's binary_logloss: 0.257587\n","[41]\ttrain's binary_logloss: 0.190121\tvalid's binary_logloss: 0.256767\n","[42]\ttrain's binary_logloss: 0.187834\tvalid's binary_logloss: 0.255625\n","[43]\ttrain's binary_logloss: 0.184909\tvalid's binary_logloss: 0.254088\n","[44]\ttrain's binary_logloss: 0.181938\tvalid's binary_logloss: 0.252355\n","[45]\ttrain's binary_logloss: 0.180014\tvalid's binary_logloss: 0.251427\n","[46]\ttrain's binary_logloss: 0.177952\tvalid's binary_logloss: 0.250039\n","[47]\ttrain's binary_logloss: 0.175486\tvalid's binary_logloss: 0.248749\n","[48]\ttrain's binary_logloss: 0.17332\tvalid's binary_logloss: 0.247716\n","[49]\ttrain's binary_logloss: 0.171233\tvalid's binary_logloss: 0.247249\n","[50]\ttrain's binary_logloss: 0.168808\tvalid's binary_logloss: 0.245712\n","[51]\ttrain's binary_logloss: 0.166901\tvalid's binary_logloss: 0.244631\n","[52]\ttrain's binary_logloss: 0.164701\tvalid's binary_logloss: 0.243459\n","[53]\ttrain's binary_logloss: 0.162987\tvalid's binary_logloss: 0.242513\n","[54]\ttrain's binary_logloss: 0.161101\tvalid's binary_logloss: 0.24179\n","[55]\ttrain's binary_logloss: 0.159227\tvalid's binary_logloss: 0.240958\n","[56]\ttrain's binary_logloss: 0.157179\tvalid's binary_logloss: 0.23982\n","[57]\ttrain's binary_logloss: 0.155645\tvalid's binary_logloss: 0.239127\n","[58]\ttrain's binary_logloss: 0.153637\tvalid's binary_logloss: 0.237746\n","[59]\ttrain's binary_logloss: 0.152156\tvalid's binary_logloss: 0.237403\n","[60]\ttrain's binary_logloss: 0.150492\tvalid's binary_logloss: 0.236752\n","[61]\ttrain's binary_logloss: 0.14833\tvalid's binary_logloss: 0.235299\n","[62]\ttrain's binary_logloss: 0.146708\tvalid's binary_logloss: 0.234711\n","[63]\ttrain's binary_logloss: 0.145146\tvalid's binary_logloss: 0.234231\n","[64]\ttrain's binary_logloss: 0.143475\tvalid's binary_logloss: 0.233571\n","[65]\ttrain's binary_logloss: 0.141857\tvalid's binary_logloss: 0.233159\n","[66]\ttrain's binary_logloss: 0.140617\tvalid's binary_logloss: 0.232806\n","[67]\ttrain's binary_logloss: 0.139349\tvalid's binary_logloss: 0.232361\n","[68]\ttrain's binary_logloss: 0.137656\tvalid's binary_logloss: 0.231805\n","[69]\ttrain's binary_logloss: 0.13622\tvalid's binary_logloss: 0.231595\n","[70]\ttrain's binary_logloss: 0.134909\tvalid's binary_logloss: 0.231123\n","[71]\ttrain's binary_logloss: 0.133365\tvalid's binary_logloss: 0.23016\n","[72]\ttrain's binary_logloss: 0.131928\tvalid's binary_logloss: 0.22954\n","[73]\ttrain's binary_logloss: 0.130743\tvalid's binary_logloss: 0.2294\n","[74]\ttrain's binary_logloss: 0.129119\tvalid's binary_logloss: 0.228424\n","[75]\ttrain's binary_logloss: 0.127482\tvalid's binary_logloss: 0.227763\n","[76]\ttrain's binary_logloss: 0.126147\tvalid's binary_logloss: 0.227717\n","[77]\ttrain's binary_logloss: 0.124614\tvalid's binary_logloss: 0.226361\n","[78]\ttrain's binary_logloss: 0.123468\tvalid's binary_logloss: 0.226123\n","[79]\ttrain's binary_logloss: 0.121894\tvalid's binary_logloss: 0.224743\n","[80]\ttrain's binary_logloss: 0.120732\tvalid's binary_logloss: 0.224614\n","[81]\ttrain's binary_logloss: 0.119582\tvalid's binary_logloss: 0.224197\n","[82]\ttrain's binary_logloss: 0.118468\tvalid's binary_logloss: 0.223638\n","[83]\ttrain's binary_logloss: 0.117333\tvalid's binary_logloss: 0.223198\n","[84]\ttrain's binary_logloss: 0.116301\tvalid's binary_logloss: 0.223096\n","[85]\ttrain's binary_logloss: 0.11519\tvalid's binary_logloss: 0.222564\n","[86]\ttrain's binary_logloss: 0.114083\tvalid's binary_logloss: 0.222174\n","[87]\ttrain's binary_logloss: 0.112979\tvalid's binary_logloss: 0.222011\n","[88]\ttrain's binary_logloss: 0.11165\tvalid's binary_logloss: 0.220694\n","[89]\ttrain's binary_logloss: 0.110383\tvalid's binary_logloss: 0.220025\n","[90]\ttrain's binary_logloss: 0.109327\tvalid's binary_logloss: 0.219546\n","[91]\ttrain's binary_logloss: 0.108284\tvalid's binary_logloss: 0.218836\n","[92]\ttrain's binary_logloss: 0.107136\tvalid's binary_logloss: 0.21845\n","[93]\ttrain's binary_logloss: 0.106131\tvalid's binary_logloss: 0.218435\n","[94]\ttrain's binary_logloss: 0.105149\tvalid's binary_logloss: 0.218246\n","[95]\ttrain's binary_logloss: 0.104193\tvalid's binary_logloss: 0.218185\n","[96]\ttrain's binary_logloss: 0.103162\tvalid's binary_logloss: 0.217907\n","[97]\ttrain's binary_logloss: 0.102197\tvalid's binary_logloss: 0.217536\n","[98]\ttrain's binary_logloss: 0.101254\tvalid's binary_logloss: 0.217335\n","[99]\ttrain's binary_logloss: 0.100266\tvalid's binary_logloss: 0.217277\n","[100]\ttrain's binary_logloss: 0.0994527\tvalid's binary_logloss: 0.217264\n","[101]\ttrain's binary_logloss: 0.0984582\tvalid's binary_logloss: 0.217421\n","[102]\ttrain's binary_logloss: 0.0975699\tvalid's binary_logloss: 0.217796\n","[103]\ttrain's binary_logloss: 0.0967684\tvalid's binary_logloss: 0.217607\n","[104]\ttrain's binary_logloss: 0.0959507\tvalid's binary_logloss: 0.217597\n","[105]\ttrain's binary_logloss: 0.0949572\tvalid's binary_logloss: 0.217595\n","[106]\ttrain's binary_logloss: 0.0939988\tvalid's binary_logloss: 0.217479\n","[107]\ttrain's binary_logloss: 0.0930806\tvalid's binary_logloss: 0.217188\n","[108]\ttrain's binary_logloss: 0.091694\tvalid's binary_logloss: 0.215699\n","[109]\ttrain's binary_logloss: 0.0909406\tvalid's binary_logloss: 0.215406\n","[110]\ttrain's binary_logloss: 0.0901812\tvalid's binary_logloss: 0.215404\n","[111]\ttrain's binary_logloss: 0.0890411\tvalid's binary_logloss: 0.214838\n","[112]\ttrain's binary_logloss: 0.0882834\tvalid's binary_logloss: 0.214922\n","[113]\ttrain's binary_logloss: 0.0875157\tvalid's binary_logloss: 0.214831\n","[114]\ttrain's binary_logloss: 0.086729\tvalid's binary_logloss: 0.214685\n","[115]\ttrain's binary_logloss: 0.0860699\tvalid's binary_logloss: 0.214667\n","[116]\ttrain's binary_logloss: 0.0853666\tvalid's binary_logloss: 0.214367\n","[117]\ttrain's binary_logloss: 0.084511\tvalid's binary_logloss: 0.214262\n","[118]\ttrain's binary_logloss: 0.0837485\tvalid's binary_logloss: 0.214242\n","[119]\ttrain's binary_logloss: 0.0829976\tvalid's binary_logloss: 0.214152\n","[120]\ttrain's binary_logloss: 0.082245\tvalid's binary_logloss: 0.213931\n","[121]\ttrain's binary_logloss: 0.0814593\tvalid's binary_logloss: 0.213654\n","[122]\ttrain's binary_logloss: 0.0806696\tvalid's binary_logloss: 0.213535\n","[123]\ttrain's binary_logloss: 0.0799197\tvalid's binary_logloss: 0.213655\n","[124]\ttrain's binary_logloss: 0.0793165\tvalid's binary_logloss: 0.213653\n","[125]\ttrain's binary_logloss: 0.0785481\tvalid's binary_logloss: 0.213472\n","[126]\ttrain's binary_logloss: 0.0779227\tvalid's binary_logloss: 0.213633\n","[127]\ttrain's binary_logloss: 0.0771509\tvalid's binary_logloss: 0.213437\n","[128]\ttrain's binary_logloss: 0.0764563\tvalid's binary_logloss: 0.212943\n","[129]\ttrain's binary_logloss: 0.0758855\tvalid's binary_logloss: 0.21303\n","[130]\ttrain's binary_logloss: 0.075231\tvalid's binary_logloss: 0.212851\n","[131]\ttrain's binary_logloss: 0.074473\tvalid's binary_logloss: 0.212178\n","[132]\ttrain's binary_logloss: 0.0738179\tvalid's binary_logloss: 0.211958\n","[133]\ttrain's binary_logloss: 0.0732225\tvalid's binary_logloss: 0.211775\n","[134]\ttrain's binary_logloss: 0.0725873\tvalid's binary_logloss: 0.211546\n","[135]\ttrain's binary_logloss: 0.071945\tvalid's binary_logloss: 0.211122\n","[136]\ttrain's binary_logloss: 0.0714022\tvalid's binary_logloss: 0.211088\n","[137]\ttrain's binary_logloss: 0.0708807\tvalid's binary_logloss: 0.211445\n","[138]\ttrain's binary_logloss: 0.0700744\tvalid's binary_logloss: 0.210563\n","[139]\ttrain's binary_logloss: 0.0695329\tvalid's binary_logloss: 0.210665\n","[140]\ttrain's binary_logloss: 0.0690487\tvalid's binary_logloss: 0.210766\n","[141]\ttrain's binary_logloss: 0.0684624\tvalid's binary_logloss: 0.210647\n","[142]\ttrain's binary_logloss: 0.0679562\tvalid's binary_logloss: 0.210728\n","[143]\ttrain's binary_logloss: 0.067303\tvalid's binary_logloss: 0.210724\n","[144]\ttrain's binary_logloss: 0.0667676\tvalid's binary_logloss: 0.211084\n","[145]\ttrain's binary_logloss: 0.0662361\tvalid's binary_logloss: 0.211097\n","[146]\ttrain's binary_logloss: 0.065744\tvalid's binary_logloss: 0.211218\n","[147]\ttrain's binary_logloss: 0.0650035\tvalid's binary_logloss: 0.21058\n","[148]\ttrain's binary_logloss: 0.0644377\tvalid's binary_logloss: 0.21052\n","[149]\ttrain's binary_logloss: 0.0639\tvalid's binary_logloss: 0.210466\n","[150]\ttrain's binary_logloss: 0.0633971\tvalid's binary_logloss: 0.21029\n","[151]\ttrain's binary_logloss: 0.0627998\tvalid's binary_logloss: 0.210587\n","[152]\ttrain's binary_logloss: 0.0623205\tvalid's binary_logloss: 0.21058\n","[153]\ttrain's binary_logloss: 0.0618731\tvalid's binary_logloss: 0.210718\n","[154]\ttrain's binary_logloss: 0.0613518\tvalid's binary_logloss: 0.210897\n","[155]\ttrain's binary_logloss: 0.0606809\tvalid's binary_logloss: 0.210511\n","[156]\ttrain's binary_logloss: 0.0602291\tvalid's binary_logloss: 0.21081\n","[157]\ttrain's binary_logloss: 0.0597253\tvalid's binary_logloss: 0.210371\n","[158]\ttrain's binary_logloss: 0.0592399\tvalid's binary_logloss: 0.210537\n","[159]\ttrain's binary_logloss: 0.0587393\tvalid's binary_logloss: 0.210385\n","[160]\ttrain's binary_logloss: 0.0583301\tvalid's binary_logloss: 0.210368\n","[161]\ttrain's binary_logloss: 0.0578369\tvalid's binary_logloss: 0.21001\n","[162]\ttrain's binary_logloss: 0.0572636\tvalid's binary_logloss: 0.210111\n","[163]\ttrain's binary_logloss: 0.0568067\tvalid's binary_logloss: 0.210214\n","[164]\ttrain's binary_logloss: 0.056341\tvalid's binary_logloss: 0.210036\n","[165]\ttrain's binary_logloss: 0.0557354\tvalid's binary_logloss: 0.209243\n","[166]\ttrain's binary_logloss: 0.0552162\tvalid's binary_logloss: 0.209214\n","[167]\ttrain's binary_logloss: 0.0548129\tvalid's binary_logloss: 0.20922\n","[168]\ttrain's binary_logloss: 0.0542959\tvalid's binary_logloss: 0.208879\n","[169]\ttrain's binary_logloss: 0.0538487\tvalid's binary_logloss: 0.209328\n","[170]\ttrain's binary_logloss: 0.053411\tvalid's binary_logloss: 0.209683\n","[171]\ttrain's binary_logloss: 0.0529817\tvalid's binary_logloss: 0.209613\n","[172]\ttrain's binary_logloss: 0.0524042\tvalid's binary_logloss: 0.209344\n","[173]\ttrain's binary_logloss: 0.051878\tvalid's binary_logloss: 0.20945\n","[174]\ttrain's binary_logloss: 0.0515024\tvalid's binary_logloss: 0.209521\n","[175]\ttrain's binary_logloss: 0.0510625\tvalid's binary_logloss: 0.209515\n","[176]\ttrain's binary_logloss: 0.0507208\tvalid's binary_logloss: 0.209295\n","[177]\ttrain's binary_logloss: 0.0502967\tvalid's binary_logloss: 0.209697\n","[178]\ttrain's binary_logloss: 0.0498493\tvalid's binary_logloss: 0.210002\n","[179]\ttrain's binary_logloss: 0.0494482\tvalid's binary_logloss: 0.209933\n","[180]\ttrain's binary_logloss: 0.0491045\tvalid's binary_logloss: 0.209851\n","[181]\ttrain's binary_logloss: 0.0487392\tvalid's binary_logloss: 0.209944\n","[182]\ttrain's binary_logloss: 0.048414\tvalid's binary_logloss: 0.209815\n","[183]\ttrain's binary_logloss: 0.0480299\tvalid's binary_logloss: 0.209765\n","[184]\ttrain's binary_logloss: 0.0476481\tvalid's binary_logloss: 0.210196\n","[185]\ttrain's binary_logloss: 0.047264\tvalid's binary_logloss: 0.210098\n","[186]\ttrain's binary_logloss: 0.0468138\tvalid's binary_logloss: 0.210026\n","[187]\ttrain's binary_logloss: 0.0463799\tvalid's binary_logloss: 0.210195\n","[188]\ttrain's binary_logloss: 0.0459321\tvalid's binary_logloss: 0.209621\n","Early stopping, best iteration is:\n","[168]\ttrain's binary_logloss: 0.0542959\tvalid's binary_logloss: 0.208879\n","[1]\ttrain's binary_logloss: 0.460683\tvalid's binary_logloss: 0.445527\n","Training until validation scores don't improve for 20 rounds.\n","[2]\ttrain's binary_logloss: 0.434914\tvalid's binary_logloss: 0.42475\n","[3]\ttrain's binary_logloss: 0.414154\tvalid's binary_logloss: 0.408448\n","[4]\ttrain's binary_logloss: 0.396952\tvalid's binary_logloss: 0.394103\n","[5]\ttrain's binary_logloss: 0.38195\tvalid's binary_logloss: 0.382367\n","[6]\ttrain's binary_logloss: 0.369146\tvalid's binary_logloss: 0.372506\n","[7]\ttrain's binary_logloss: 0.357112\tvalid's binary_logloss: 0.364028\n","[8]\ttrain's binary_logloss: 0.346977\tvalid's binary_logloss: 0.357331\n","[9]\ttrain's binary_logloss: 0.337622\tvalid's binary_logloss: 0.350197\n","[10]\ttrain's binary_logloss: 0.329478\tvalid's binary_logloss: 0.344383\n","[11]\ttrain's binary_logloss: 0.321132\tvalid's binary_logloss: 0.338466\n","[12]\ttrain's binary_logloss: 0.313617\tvalid's binary_logloss: 0.332766\n","[13]\ttrain's binary_logloss: 0.30608\tvalid's binary_logloss: 0.326439\n","[14]\ttrain's binary_logloss: 0.299119\tvalid's binary_logloss: 0.321705\n","[15]\ttrain's binary_logloss: 0.292493\tvalid's binary_logloss: 0.316989\n","[16]\ttrain's binary_logloss: 0.285813\tvalid's binary_logloss: 0.311977\n","[17]\ttrain's binary_logloss: 0.27985\tvalid's binary_logloss: 0.308307\n","[18]\ttrain's binary_logloss: 0.274492\tvalid's binary_logloss: 0.305441\n","[19]\ttrain's binary_logloss: 0.269809\tvalid's binary_logloss: 0.301833\n","[20]\ttrain's binary_logloss: 0.264112\tvalid's binary_logloss: 0.298228\n","[21]\ttrain's binary_logloss: 0.259142\tvalid's binary_logloss: 0.294624\n","[22]\ttrain's binary_logloss: 0.254988\tvalid's binary_logloss: 0.291626\n","[23]\ttrain's binary_logloss: 0.2509\tvalid's binary_logloss: 0.289124\n","[24]\ttrain's binary_logloss: 0.246887\tvalid's binary_logloss: 0.28657\n","[25]\ttrain's binary_logloss: 0.24273\tvalid's binary_logloss: 0.284375\n","[26]\ttrain's binary_logloss: 0.238828\tvalid's binary_logloss: 0.282064\n","[27]\ttrain's binary_logloss: 0.234758\tvalid's binary_logloss: 0.279011\n","[28]\ttrain's binary_logloss: 0.231367\tvalid's binary_logloss: 0.27719\n","[29]\ttrain's binary_logloss: 0.226885\tvalid's binary_logloss: 0.27392\n","[30]\ttrain's binary_logloss: 0.223622\tvalid's binary_logloss: 0.271819\n","[31]\ttrain's binary_logloss: 0.220823\tvalid's binary_logloss: 0.26999\n","[32]\ttrain's binary_logloss: 0.218027\tvalid's binary_logloss: 0.268667\n","[33]\ttrain's binary_logloss: 0.214645\tvalid's binary_logloss: 0.266851\n","[34]\ttrain's binary_logloss: 0.21106\tvalid's binary_logloss: 0.263664\n","[35]\ttrain's binary_logloss: 0.208429\tvalid's binary_logloss: 0.262366\n","[36]\ttrain's binary_logloss: 0.205435\tvalid's binary_logloss: 0.260092\n","[37]\ttrain's binary_logloss: 0.202718\tvalid's binary_logloss: 0.258452\n","[38]\ttrain's binary_logloss: 0.199477\tvalid's binary_logloss: 0.255889\n","[39]\ttrain's binary_logloss: 0.196729\tvalid's binary_logloss: 0.254161\n","[40]\ttrain's binary_logloss: 0.193862\tvalid's binary_logloss: 0.252688\n","[41]\ttrain's binary_logloss: 0.191261\tvalid's binary_logloss: 0.250924\n","[42]\ttrain's binary_logloss: 0.188936\tvalid's binary_logloss: 0.250589\n","[43]\ttrain's binary_logloss: 0.186726\tvalid's binary_logloss: 0.249751\n","[44]\ttrain's binary_logloss: 0.184547\tvalid's binary_logloss: 0.248667\n","[45]\ttrain's binary_logloss: 0.182229\tvalid's binary_logloss: 0.247304\n","[46]\ttrain's binary_logloss: 0.180278\tvalid's binary_logloss: 0.24663\n","[47]\ttrain's binary_logloss: 0.178227\tvalid's binary_logloss: 0.245724\n","[48]\ttrain's binary_logloss: 0.17598\tvalid's binary_logloss: 0.244177\n","[49]\ttrain's binary_logloss: 0.174074\tvalid's binary_logloss: 0.243811\n","[50]\ttrain's binary_logloss: 0.171338\tvalid's binary_logloss: 0.24192\n","[51]\ttrain's binary_logloss: 0.169476\tvalid's binary_logloss: 0.241339\n","[52]\ttrain's binary_logloss: 0.167169\tvalid's binary_logloss: 0.239601\n","[53]\ttrain's binary_logloss: 0.165316\tvalid's binary_logloss: 0.238659\n","[54]\ttrain's binary_logloss: 0.163015\tvalid's binary_logloss: 0.236957\n","[55]\ttrain's binary_logloss: 0.160341\tvalid's binary_logloss: 0.234712\n","[56]\ttrain's binary_logloss: 0.158558\tvalid's binary_logloss: 0.234069\n","[57]\ttrain's binary_logloss: 0.156989\tvalid's binary_logloss: 0.23333\n","[58]\ttrain's binary_logloss: 0.155397\tvalid's binary_logloss: 0.2327\n","[59]\ttrain's binary_logloss: 0.153571\tvalid's binary_logloss: 0.231853\n","[60]\ttrain's binary_logloss: 0.151882\tvalid's binary_logloss: 0.231119\n","[61]\ttrain's binary_logloss: 0.150035\tvalid's binary_logloss: 0.229544\n","[62]\ttrain's binary_logloss: 0.148515\tvalid's binary_logloss: 0.229169\n","[63]\ttrain's binary_logloss: 0.146908\tvalid's binary_logloss: 0.228923\n","[64]\ttrain's binary_logloss: 0.145537\tvalid's binary_logloss: 0.228752\n","[65]\ttrain's binary_logloss: 0.144203\tvalid's binary_logloss: 0.228824\n","[66]\ttrain's binary_logloss: 0.1428\tvalid's binary_logloss: 0.228232\n","[67]\ttrain's binary_logloss: 0.141316\tvalid's binary_logloss: 0.227693\n","[68]\ttrain's binary_logloss: 0.139916\tvalid's binary_logloss: 0.226982\n","[69]\ttrain's binary_logloss: 0.138496\tvalid's binary_logloss: 0.226912\n","[70]\ttrain's binary_logloss: 0.136849\tvalid's binary_logloss: 0.225778\n","[71]\ttrain's binary_logloss: 0.135715\tvalid's binary_logloss: 0.225521\n","[72]\ttrain's binary_logloss: 0.13433\tvalid's binary_logloss: 0.224983\n","[73]\ttrain's binary_logloss: 0.132912\tvalid's binary_logloss: 0.224385\n","[74]\ttrain's binary_logloss: 0.13138\tvalid's binary_logloss: 0.223778\n","[75]\ttrain's binary_logloss: 0.130124\tvalid's binary_logloss: 0.222861\n","[76]\ttrain's binary_logloss: 0.128931\tvalid's binary_logloss: 0.222522\n","[77]\ttrain's binary_logloss: 0.127775\tvalid's binary_logloss: 0.222622\n","[78]\ttrain's binary_logloss: 0.126455\tvalid's binary_logloss: 0.222063\n","[79]\ttrain's binary_logloss: 0.124851\tvalid's binary_logloss: 0.221238\n","[80]\ttrain's binary_logloss: 0.123553\tvalid's binary_logloss: 0.220734\n","[81]\ttrain's binary_logloss: 0.122352\tvalid's binary_logloss: 0.220331\n","[82]\ttrain's binary_logloss: 0.121223\tvalid's binary_logloss: 0.219743\n","[83]\ttrain's binary_logloss: 0.1202\tvalid's binary_logloss: 0.220056\n","[84]\ttrain's binary_logloss: 0.119073\tvalid's binary_logloss: 0.219459\n","[85]\ttrain's binary_logloss: 0.117989\tvalid's binary_logloss: 0.219275\n","[86]\ttrain's binary_logloss: 0.116666\tvalid's binary_logloss: 0.218472\n","[87]\ttrain's binary_logloss: 0.115579\tvalid's binary_logloss: 0.218394\n","[88]\ttrain's binary_logloss: 0.114462\tvalid's binary_logloss: 0.217985\n","[89]\ttrain's binary_logloss: 0.113412\tvalid's binary_logloss: 0.217221\n","[90]\ttrain's binary_logloss: 0.112221\tvalid's binary_logloss: 0.217117\n","[91]\ttrain's binary_logloss: 0.111297\tvalid's binary_logloss: 0.2171\n","[92]\ttrain's binary_logloss: 0.110277\tvalid's binary_logloss: 0.216819\n","[93]\ttrain's binary_logloss: 0.109143\tvalid's binary_logloss: 0.216398\n","[94]\ttrain's binary_logloss: 0.107932\tvalid's binary_logloss: 0.215407\n","[95]\ttrain's binary_logloss: 0.106959\tvalid's binary_logloss: 0.215293\n","[96]\ttrain's binary_logloss: 0.106024\tvalid's binary_logloss: 0.214868\n","[97]\ttrain's binary_logloss: 0.105188\tvalid's binary_logloss: 0.214679\n","[98]\ttrain's binary_logloss: 0.104204\tvalid's binary_logloss: 0.214062\n","[99]\ttrain's binary_logloss: 0.103322\tvalid's binary_logloss: 0.214188\n","[100]\ttrain's binary_logloss: 0.102367\tvalid's binary_logloss: 0.214592\n","[101]\ttrain's binary_logloss: 0.101525\tvalid's binary_logloss: 0.214412\n","[102]\ttrain's binary_logloss: 0.100693\tvalid's binary_logloss: 0.214393\n","[103]\ttrain's binary_logloss: 0.0996769\tvalid's binary_logloss: 0.213552\n","[104]\ttrain's binary_logloss: 0.0987921\tvalid's binary_logloss: 0.213159\n","[105]\ttrain's binary_logloss: 0.0979626\tvalid's binary_logloss: 0.213172\n","[106]\ttrain's binary_logloss: 0.0971136\tvalid's binary_logloss: 0.212823\n","[107]\ttrain's binary_logloss: 0.0962241\tvalid's binary_logloss: 0.212642\n","[108]\ttrain's binary_logloss: 0.0953245\tvalid's binary_logloss: 0.212439\n","[109]\ttrain's binary_logloss: 0.0945541\tvalid's binary_logloss: 0.212131\n","[110]\ttrain's binary_logloss: 0.0938192\tvalid's binary_logloss: 0.212163\n","[111]\ttrain's binary_logloss: 0.0929698\tvalid's binary_logloss: 0.212054\n","[112]\ttrain's binary_logloss: 0.0922092\tvalid's binary_logloss: 0.21196\n","[113]\ttrain's binary_logloss: 0.0914111\tvalid's binary_logloss: 0.211436\n","[114]\ttrain's binary_logloss: 0.0906028\tvalid's binary_logloss: 0.211638\n","[115]\ttrain's binary_logloss: 0.0898698\tvalid's binary_logloss: 0.211602\n","[116]\ttrain's binary_logloss: 0.089062\tvalid's binary_logloss: 0.210803\n","[117]\ttrain's binary_logloss: 0.0882604\tvalid's binary_logloss: 0.210498\n","[118]\ttrain's binary_logloss: 0.08759\tvalid's binary_logloss: 0.21055\n","[119]\ttrain's binary_logloss: 0.0868173\tvalid's binary_logloss: 0.210417\n","[120]\ttrain's binary_logloss: 0.0860495\tvalid's binary_logloss: 0.210216\n","[121]\ttrain's binary_logloss: 0.0853698\tvalid's binary_logloss: 0.20999\n","[122]\ttrain's binary_logloss: 0.0845981\tvalid's binary_logloss: 0.209956\n","[123]\ttrain's binary_logloss: 0.0839034\tvalid's binary_logloss: 0.210246\n","[124]\ttrain's binary_logloss: 0.0828611\tvalid's binary_logloss: 0.209249\n","[125]\ttrain's binary_logloss: 0.0820338\tvalid's binary_logloss: 0.208907\n","[126]\ttrain's binary_logloss: 0.0813352\tvalid's binary_logloss: 0.209101\n","[127]\ttrain's binary_logloss: 0.0806114\tvalid's binary_logloss: 0.20853\n","[128]\ttrain's binary_logloss: 0.0798895\tvalid's binary_logloss: 0.208417\n","[129]\ttrain's binary_logloss: 0.079271\tvalid's binary_logloss: 0.208364\n","[130]\ttrain's binary_logloss: 0.0785295\tvalid's binary_logloss: 0.208118\n","[131]\ttrain's binary_logloss: 0.0779091\tvalid's binary_logloss: 0.208241\n","[132]\ttrain's binary_logloss: 0.0770903\tvalid's binary_logloss: 0.208079\n","[133]\ttrain's binary_logloss: 0.0764292\tvalid's binary_logloss: 0.207978\n","[134]\ttrain's binary_logloss: 0.0757181\tvalid's binary_logloss: 0.207806\n","[135]\ttrain's binary_logloss: 0.0751052\tvalid's binary_logloss: 0.2079\n","[136]\ttrain's binary_logloss: 0.0745095\tvalid's binary_logloss: 0.207807\n","[137]\ttrain's binary_logloss: 0.0739465\tvalid's binary_logloss: 0.208016\n","[138]\ttrain's binary_logloss: 0.073327\tvalid's binary_logloss: 0.208056\n","[139]\ttrain's binary_logloss: 0.0727128\tvalid's binary_logloss: 0.207856\n","[140]\ttrain's binary_logloss: 0.072085\tvalid's binary_logloss: 0.207632\n","[141]\ttrain's binary_logloss: 0.0714982\tvalid's binary_logloss: 0.207571\n","[142]\ttrain's binary_logloss: 0.0708236\tvalid's binary_logloss: 0.207341\n","[143]\ttrain's binary_logloss: 0.0702457\tvalid's binary_logloss: 0.207194\n","[144]\ttrain's binary_logloss: 0.0695902\tvalid's binary_logloss: 0.207408\n","[145]\ttrain's binary_logloss: 0.0690746\tvalid's binary_logloss: 0.207122\n","[146]\ttrain's binary_logloss: 0.0685459\tvalid's binary_logloss: 0.207402\n","[147]\ttrain's binary_logloss: 0.0680424\tvalid's binary_logloss: 0.207596\n","[148]\ttrain's binary_logloss: 0.0674316\tvalid's binary_logloss: 0.2074\n","[149]\ttrain's binary_logloss: 0.0668443\tvalid's binary_logloss: 0.207573\n","[150]\ttrain's binary_logloss: 0.0661905\tvalid's binary_logloss: 0.207182\n","[151]\ttrain's binary_logloss: 0.06555\tvalid's binary_logloss: 0.207074\n","[152]\ttrain's binary_logloss: 0.064885\tvalid's binary_logloss: 0.206522\n","[153]\ttrain's binary_logloss: 0.0643136\tvalid's binary_logloss: 0.206224\n","[154]\ttrain's binary_logloss: 0.0637548\tvalid's binary_logloss: 0.206395\n","[155]\ttrain's binary_logloss: 0.0632317\tvalid's binary_logloss: 0.206437\n","[156]\ttrain's binary_logloss: 0.0628016\tvalid's binary_logloss: 0.206686\n","[157]\ttrain's binary_logloss: 0.0623112\tvalid's binary_logloss: 0.206906\n","[158]\ttrain's binary_logloss: 0.0616407\tvalid's binary_logloss: 0.206388\n","[159]\ttrain's binary_logloss: 0.0610972\tvalid's binary_logloss: 0.206153\n","[160]\ttrain's binary_logloss: 0.0606088\tvalid's binary_logloss: 0.205971\n","[161]\ttrain's binary_logloss: 0.060194\tvalid's binary_logloss: 0.205939\n","[162]\ttrain's binary_logloss: 0.0597466\tvalid's binary_logloss: 0.206112\n","[163]\ttrain's binary_logloss: 0.0592514\tvalid's binary_logloss: 0.206349\n","[164]\ttrain's binary_logloss: 0.0587217\tvalid's binary_logloss: 0.206368\n","[165]\ttrain's binary_logloss: 0.0582007\tvalid's binary_logloss: 0.206436\n","[166]\ttrain's binary_logloss: 0.0577437\tvalid's binary_logloss: 0.206583\n","[167]\ttrain's binary_logloss: 0.0573102\tvalid's binary_logloss: 0.206384\n","[168]\ttrain's binary_logloss: 0.0568791\tvalid's binary_logloss: 0.206409\n","[169]\ttrain's binary_logloss: 0.0564571\tvalid's binary_logloss: 0.20616\n","[170]\ttrain's binary_logloss: 0.0559632\tvalid's binary_logloss: 0.20608\n","[171]\ttrain's binary_logloss: 0.0555476\tvalid's binary_logloss: 0.206275\n","[172]\ttrain's binary_logloss: 0.0551783\tvalid's binary_logloss: 0.206496\n","[173]\ttrain's binary_logloss: 0.0547616\tvalid's binary_logloss: 0.206501\n","[174]\ttrain's binary_logloss: 0.0542995\tvalid's binary_logloss: 0.206525\n","[175]\ttrain's binary_logloss: 0.0538168\tvalid's binary_logloss: 0.206369\n","[176]\ttrain's binary_logloss: 0.0533765\tvalid's binary_logloss: 0.206358\n","[177]\ttrain's binary_logloss: 0.0529351\tvalid's binary_logloss: 0.205973\n","[178]\ttrain's binary_logloss: 0.0525255\tvalid's binary_logloss: 0.205915\n","[179]\ttrain's binary_logloss: 0.0521267\tvalid's binary_logloss: 0.206219\n","[180]\ttrain's binary_logloss: 0.0517134\tvalid's binary_logloss: 0.206449\n","[181]\ttrain's binary_logloss: 0.051303\tvalid's binary_logloss: 0.2062\n","[182]\ttrain's binary_logloss: 0.0508949\tvalid's binary_logloss: 0.205887\n","[183]\ttrain's binary_logloss: 0.0504518\tvalid's binary_logloss: 0.205923\n","[184]\ttrain's binary_logloss: 0.0500408\tvalid's binary_logloss: 0.206252\n","[185]\ttrain's binary_logloss: 0.049642\tvalid's binary_logloss: 0.206259\n","[186]\ttrain's binary_logloss: 0.0491502\tvalid's binary_logloss: 0.205699\n","[187]\ttrain's binary_logloss: 0.0487909\tvalid's binary_logloss: 0.205836\n","[188]\ttrain's binary_logloss: 0.048335\tvalid's binary_logloss: 0.205635\n","[189]\ttrain's binary_logloss: 0.0479115\tvalid's binary_logloss: 0.20554\n","[190]\ttrain's binary_logloss: 0.047528\tvalid's binary_logloss: 0.20574\n","[191]\ttrain's binary_logloss: 0.0471562\tvalid's binary_logloss: 0.205948\n","[192]\ttrain's binary_logloss: 0.0468205\tvalid's binary_logloss: 0.206379\n","[193]\ttrain's binary_logloss: 0.0464705\tvalid's binary_logloss: 0.206353\n","[194]\ttrain's binary_logloss: 0.0460823\tvalid's binary_logloss: 0.206239\n","[195]\ttrain's binary_logloss: 0.0457575\tvalid's binary_logloss: 0.206155\n","[196]\ttrain's binary_logloss: 0.0453154\tvalid's binary_logloss: 0.205807\n","[197]\ttrain's binary_logloss: 0.0449164\tvalid's binary_logloss: 0.205744\n","[198]\ttrain's binary_logloss: 0.0445201\tvalid's binary_logloss: 0.205853\n","[199]\ttrain's binary_logloss: 0.0442044\tvalid's binary_logloss: 0.205992\n","[200]\ttrain's binary_logloss: 0.0438734\tvalid's binary_logloss: 0.206096\n","[201]\ttrain's binary_logloss: 0.0435469\tvalid's binary_logloss: 0.206161\n","[202]\ttrain's binary_logloss: 0.0432511\tvalid's binary_logloss: 0.206206\n","[203]\ttrain's binary_logloss: 0.0428166\tvalid's binary_logloss: 0.20557\n","[204]\ttrain's binary_logloss: 0.0424848\tvalid's binary_logloss: 0.205559\n","[205]\ttrain's binary_logloss: 0.0421644\tvalid's binary_logloss: 0.205821\n","[206]\ttrain's binary_logloss: 0.041818\tvalid's binary_logloss: 0.205474\n","[207]\ttrain's binary_logloss: 0.04149\tvalid's binary_logloss: 0.205281\n","[208]\ttrain's binary_logloss: 0.0412097\tvalid's binary_logloss: 0.205409\n","[209]\ttrain's binary_logloss: 0.0408605\tvalid's binary_logloss: 0.205535\n","[210]\ttrain's binary_logloss: 0.0405736\tvalid's binary_logloss: 0.205707\n","[211]\ttrain's binary_logloss: 0.0402524\tvalid's binary_logloss: 0.205597\n","[212]\ttrain's binary_logloss: 0.039941\tvalid's binary_logloss: 0.205609\n","[213]\ttrain's binary_logloss: 0.0396435\tvalid's binary_logloss: 0.205677\n","[214]\ttrain's binary_logloss: 0.0393177\tvalid's binary_logloss: 0.205431\n","[215]\ttrain's binary_logloss: 0.0390359\tvalid's binary_logloss: 0.205258\n","[216]\ttrain's binary_logloss: 0.0387535\tvalid's binary_logloss: 0.20512\n","[217]\ttrain's binary_logloss: 0.038456\tvalid's binary_logloss: 0.205075\n","[218]\ttrain's binary_logloss: 0.0381515\tvalid's binary_logloss: 0.20503\n","[219]\ttrain's binary_logloss: 0.0377805\tvalid's binary_logloss: 0.204575\n","[220]\ttrain's binary_logloss: 0.0374968\tvalid's binary_logloss: 0.204843\n","[221]\ttrain's binary_logloss: 0.0371995\tvalid's binary_logloss: 0.204792\n","[222]\ttrain's binary_logloss: 0.0368931\tvalid's binary_logloss: 0.204982\n","[223]\ttrain's binary_logloss: 0.036614\tvalid's binary_logloss: 0.204522\n","[224]\ttrain's binary_logloss: 0.0363316\tvalid's binary_logloss: 0.204709\n","[225]\ttrain's binary_logloss: 0.0360306\tvalid's binary_logloss: 0.204856\n","[226]\ttrain's binary_logloss: 0.0357267\tvalid's binary_logloss: 0.20483\n","[227]\ttrain's binary_logloss: 0.0354599\tvalid's binary_logloss: 0.205134\n","[228]\ttrain's binary_logloss: 0.0352127\tvalid's binary_logloss: 0.205125\n","[229]\ttrain's binary_logloss: 0.034927\tvalid's binary_logloss: 0.205492\n","[230]\ttrain's binary_logloss: 0.0346707\tvalid's binary_logloss: 0.205785\n","[231]\ttrain's binary_logloss: 0.0344024\tvalid's binary_logloss: 0.205871\n","[232]\ttrain's binary_logloss: 0.0341246\tvalid's binary_logloss: 0.206144\n","[233]\ttrain's binary_logloss: 0.0337964\tvalid's binary_logloss: 0.206158\n","[234]\ttrain's binary_logloss: 0.0335775\tvalid's binary_logloss: 0.206326\n","[235]\ttrain's binary_logloss: 0.0333606\tvalid's binary_logloss: 0.206182\n","[236]\ttrain's binary_logloss: 0.0330641\tvalid's binary_logloss: 0.205972\n","[237]\ttrain's binary_logloss: 0.032807\tvalid's binary_logloss: 0.206021\n","[238]\ttrain's binary_logloss: 0.0325501\tvalid's binary_logloss: 0.206328\n","[239]\ttrain's binary_logloss: 0.032314\tvalid's binary_logloss: 0.206491\n","[240]\ttrain's binary_logloss: 0.0320739\tvalid's binary_logloss: 0.206316\n","[241]\ttrain's binary_logloss: 0.0318359\tvalid's binary_logloss: 0.206232\n","[242]\ttrain's binary_logloss: 0.0316266\tvalid's binary_logloss: 0.206095\n","[243]\ttrain's binary_logloss: 0.031379\tvalid's binary_logloss: 0.206006\n","Early stopping, best iteration is:\n","[223]\ttrain's binary_logloss: 0.036614\tvalid's binary_logloss: 0.204522\n","[1]\ttrain's binary_logloss: 0.454514\tvalid's binary_logloss: 0.462302\n","Training until validation scores don't improve for 20 rounds.\n","[2]\ttrain's binary_logloss: 0.430155\tvalid's binary_logloss: 0.439446\n","[3]\ttrain's binary_logloss: 0.411274\tvalid's binary_logloss: 0.420192\n","[4]\ttrain's binary_logloss: 0.395891\tvalid's binary_logloss: 0.405234\n","[5]\ttrain's binary_logloss: 0.380893\tvalid's binary_logloss: 0.391113\n","[6]\ttrain's binary_logloss: 0.3687\tvalid's binary_logloss: 0.381125\n","[7]\ttrain's binary_logloss: 0.357678\tvalid's binary_logloss: 0.371047\n","[8]\ttrain's binary_logloss: 0.347273\tvalid's binary_logloss: 0.362708\n","[9]\ttrain's binary_logloss: 0.3384\tvalid's binary_logloss: 0.355273\n","[10]\ttrain's binary_logloss: 0.329459\tvalid's binary_logloss: 0.346679\n","[11]\ttrain's binary_logloss: 0.321347\tvalid's binary_logloss: 0.340116\n","[12]\ttrain's binary_logloss: 0.314839\tvalid's binary_logloss: 0.334649\n","[13]\ttrain's binary_logloss: 0.308432\tvalid's binary_logloss: 0.328457\n","[14]\ttrain's binary_logloss: 0.302163\tvalid's binary_logloss: 0.323778\n","[15]\ttrain's binary_logloss: 0.295853\tvalid's binary_logloss: 0.318936\n","[16]\ttrain's binary_logloss: 0.289292\tvalid's binary_logloss: 0.314108\n","[17]\ttrain's binary_logloss: 0.28355\tvalid's binary_logloss: 0.309519\n","[18]\ttrain's binary_logloss: 0.278249\tvalid's binary_logloss: 0.305123\n","[19]\ttrain's binary_logloss: 0.27248\tvalid's binary_logloss: 0.299955\n","[20]\ttrain's binary_logloss: 0.267242\tvalid's binary_logloss: 0.295732\n","[21]\ttrain's binary_logloss: 0.262732\tvalid's binary_logloss: 0.292204\n","[22]\ttrain's binary_logloss: 0.258047\tvalid's binary_logloss: 0.289008\n","[23]\ttrain's binary_logloss: 0.252391\tvalid's binary_logloss: 0.284907\n","[24]\ttrain's binary_logloss: 0.247533\tvalid's binary_logloss: 0.281473\n","[25]\ttrain's binary_logloss: 0.243537\tvalid's binary_logloss: 0.278999\n","[26]\ttrain's binary_logloss: 0.238803\tvalid's binary_logloss: 0.275479\n","[27]\ttrain's binary_logloss: 0.235009\tvalid's binary_logloss: 0.27302\n","[28]\ttrain's binary_logloss: 0.230733\tvalid's binary_logloss: 0.27064\n","[29]\ttrain's binary_logloss: 0.226691\tvalid's binary_logloss: 0.267774\n","[30]\ttrain's binary_logloss: 0.222935\tvalid's binary_logloss: 0.265414\n","[31]\ttrain's binary_logloss: 0.220249\tvalid's binary_logloss: 0.264213\n","[32]\ttrain's binary_logloss: 0.217233\tvalid's binary_logloss: 0.262515\n","[33]\ttrain's binary_logloss: 0.213647\tvalid's binary_logloss: 0.260202\n","[34]\ttrain's binary_logloss: 0.210791\tvalid's binary_logloss: 0.259205\n","[35]\ttrain's binary_logloss: 0.208228\tvalid's binary_logloss: 0.258056\n","[36]\ttrain's binary_logloss: 0.205292\tvalid's binary_logloss: 0.256281\n","[37]\ttrain's binary_logloss: 0.201464\tvalid's binary_logloss: 0.253669\n","[38]\ttrain's binary_logloss: 0.199071\tvalid's binary_logloss: 0.2523\n","[39]\ttrain's binary_logloss: 0.196644\tvalid's binary_logloss: 0.250643\n","[40]\ttrain's binary_logloss: 0.193564\tvalid's binary_logloss: 0.249192\n","[41]\ttrain's binary_logloss: 0.191324\tvalid's binary_logloss: 0.247782\n","[42]\ttrain's binary_logloss: 0.188767\tvalid's binary_logloss: 0.246162\n","[43]\ttrain's binary_logloss: 0.186339\tvalid's binary_logloss: 0.244493\n","[44]\ttrain's binary_logloss: 0.183478\tvalid's binary_logloss: 0.243403\n","[45]\ttrain's binary_logloss: 0.181354\tvalid's binary_logloss: 0.242426\n","[46]\ttrain's binary_logloss: 0.179377\tvalid's binary_logloss: 0.241998\n","[47]\ttrain's binary_logloss: 0.177363\tvalid's binary_logloss: 0.240868\n","[48]\ttrain's binary_logloss: 0.175493\tvalid's binary_logloss: 0.240088\n","[49]\ttrain's binary_logloss: 0.173663\tvalid's binary_logloss: 0.238919\n","[50]\ttrain's binary_logloss: 0.170515\tvalid's binary_logloss: 0.236455\n","[51]\ttrain's binary_logloss: 0.168623\tvalid's binary_logloss: 0.235594\n","[52]\ttrain's binary_logloss: 0.166865\tvalid's binary_logloss: 0.234834\n","[53]\ttrain's binary_logloss: 0.165164\tvalid's binary_logloss: 0.234259\n","[54]\ttrain's binary_logloss: 0.163417\tvalid's binary_logloss: 0.233238\n","[55]\ttrain's binary_logloss: 0.161513\tvalid's binary_logloss: 0.232266\n","[56]\ttrain's binary_logloss: 0.159754\tvalid's binary_logloss: 0.231636\n","[57]\ttrain's binary_logloss: 0.157388\tvalid's binary_logloss: 0.230193\n","[58]\ttrain's binary_logloss: 0.155472\tvalid's binary_logloss: 0.228464\n","[59]\ttrain's binary_logloss: 0.153031\tvalid's binary_logloss: 0.226847\n","[60]\ttrain's binary_logloss: 0.151532\tvalid's binary_logloss: 0.226568\n","[61]\ttrain's binary_logloss: 0.15017\tvalid's binary_logloss: 0.226009\n","[62]\ttrain's binary_logloss: 0.148635\tvalid's binary_logloss: 0.224963\n","[63]\ttrain's binary_logloss: 0.146553\tvalid's binary_logloss: 0.224136\n","[64]\ttrain's binary_logloss: 0.145045\tvalid's binary_logloss: 0.223451\n","[65]\ttrain's binary_logloss: 0.143727\tvalid's binary_logloss: 0.222807\n","[66]\ttrain's binary_logloss: 0.142241\tvalid's binary_logloss: 0.221845\n","[67]\ttrain's binary_logloss: 0.140315\tvalid's binary_logloss: 0.220445\n","[68]\ttrain's binary_logloss: 0.138894\tvalid's binary_logloss: 0.219881\n","[69]\ttrain's binary_logloss: 0.137552\tvalid's binary_logloss: 0.219157\n","[70]\ttrain's binary_logloss: 0.136268\tvalid's binary_logloss: 0.218572\n","[71]\ttrain's binary_logloss: 0.134878\tvalid's binary_logloss: 0.218023\n","[72]\ttrain's binary_logloss: 0.133489\tvalid's binary_logloss: 0.217522\n","[73]\ttrain's binary_logloss: 0.132288\tvalid's binary_logloss: 0.216799\n","[74]\ttrain's binary_logloss: 0.13093\tvalid's binary_logloss: 0.216275\n","[75]\ttrain's binary_logloss: 0.129458\tvalid's binary_logloss: 0.215622\n","[76]\ttrain's binary_logloss: 0.128267\tvalid's binary_logloss: 0.215018\n","[77]\ttrain's binary_logloss: 0.127159\tvalid's binary_logloss: 0.214359\n","[78]\ttrain's binary_logloss: 0.125919\tvalid's binary_logloss: 0.214125\n","[79]\ttrain's binary_logloss: 0.124661\tvalid's binary_logloss: 0.213362\n","[80]\ttrain's binary_logloss: 0.123555\tvalid's binary_logloss: 0.212817\n","[81]\ttrain's binary_logloss: 0.122301\tvalid's binary_logloss: 0.212443\n","[82]\ttrain's binary_logloss: 0.121177\tvalid's binary_logloss: 0.212212\n","[83]\ttrain's binary_logloss: 0.120084\tvalid's binary_logloss: 0.21142\n","[84]\ttrain's binary_logloss: 0.118992\tvalid's binary_logloss: 0.211128\n","[85]\ttrain's binary_logloss: 0.117695\tvalid's binary_logloss: 0.210816\n","[86]\ttrain's binary_logloss: 0.116487\tvalid's binary_logloss: 0.210593\n","[87]\ttrain's binary_logloss: 0.115484\tvalid's binary_logloss: 0.210566\n","[88]\ttrain's binary_logloss: 0.114101\tvalid's binary_logloss: 0.209503\n","[89]\ttrain's binary_logloss: 0.112966\tvalid's binary_logloss: 0.209238\n","[90]\ttrain's binary_logloss: 0.111862\tvalid's binary_logloss: 0.208966\n","[91]\ttrain's binary_logloss: 0.110951\tvalid's binary_logloss: 0.208842\n","[92]\ttrain's binary_logloss: 0.110017\tvalid's binary_logloss: 0.208357\n","[93]\ttrain's binary_logloss: 0.109058\tvalid's binary_logloss: 0.208417\n","[94]\ttrain's binary_logloss: 0.108028\tvalid's binary_logloss: 0.208343\n","[95]\ttrain's binary_logloss: 0.106969\tvalid's binary_logloss: 0.207906\n","[96]\ttrain's binary_logloss: 0.106108\tvalid's binary_logloss: 0.207697\n","[97]\ttrain's binary_logloss: 0.105098\tvalid's binary_logloss: 0.207368\n","[98]\ttrain's binary_logloss: 0.103745\tvalid's binary_logloss: 0.2068\n","[99]\ttrain's binary_logloss: 0.102843\tvalid's binary_logloss: 0.206963\n","[100]\ttrain's binary_logloss: 0.101981\tvalid's binary_logloss: 0.206682\n","[101]\ttrain's binary_logloss: 0.101164\tvalid's binary_logloss: 0.20639\n","[102]\ttrain's binary_logloss: 0.10026\tvalid's binary_logloss: 0.206262\n","[103]\ttrain's binary_logloss: 0.0994869\tvalid's binary_logloss: 0.206489\n","[104]\ttrain's binary_logloss: 0.098597\tvalid's binary_logloss: 0.206083\n","[105]\ttrain's binary_logloss: 0.0976799\tvalid's binary_logloss: 0.205772\n","[106]\ttrain's binary_logloss: 0.0967338\tvalid's binary_logloss: 0.205662\n","[107]\ttrain's binary_logloss: 0.0956768\tvalid's binary_logloss: 0.204761\n","[108]\ttrain's binary_logloss: 0.0948297\tvalid's binary_logloss: 0.204843\n","[109]\ttrain's binary_logloss: 0.0936944\tvalid's binary_logloss: 0.204499\n","[110]\ttrain's binary_logloss: 0.0929353\tvalid's binary_logloss: 0.203948\n","[111]\ttrain's binary_logloss: 0.0920814\tvalid's binary_logloss: 0.203785\n","[112]\ttrain's binary_logloss: 0.090949\tvalid's binary_logloss: 0.20276\n","[113]\ttrain's binary_logloss: 0.0901392\tvalid's binary_logloss: 0.202706\n","[114]\ttrain's binary_logloss: 0.0892906\tvalid's binary_logloss: 0.202215\n","[115]\ttrain's binary_logloss: 0.0883389\tvalid's binary_logloss: 0.201463\n","[116]\ttrain's binary_logloss: 0.0876683\tvalid's binary_logloss: 0.200936\n","[117]\ttrain's binary_logloss: 0.0867757\tvalid's binary_logloss: 0.200539\n","[118]\ttrain's binary_logloss: 0.0860672\tvalid's binary_logloss: 0.200273\n","[119]\ttrain's binary_logloss: 0.084845\tvalid's binary_logloss: 0.199365\n","[120]\ttrain's binary_logloss: 0.0840316\tvalid's binary_logloss: 0.198898\n","[121]\ttrain's binary_logloss: 0.0834098\tvalid's binary_logloss: 0.198746\n","[122]\ttrain's binary_logloss: 0.0827689\tvalid's binary_logloss: 0.198654\n","[123]\ttrain's binary_logloss: 0.082029\tvalid's binary_logloss: 0.198689\n","[124]\ttrain's binary_logloss: 0.0812537\tvalid's binary_logloss: 0.198743\n","[125]\ttrain's binary_logloss: 0.080635\tvalid's binary_logloss: 0.198836\n","[126]\ttrain's binary_logloss: 0.0798253\tvalid's binary_logloss: 0.19884\n","[127]\ttrain's binary_logloss: 0.0791449\tvalid's binary_logloss: 0.198822\n","[128]\ttrain's binary_logloss: 0.0785048\tvalid's binary_logloss: 0.198323\n","[129]\ttrain's binary_logloss: 0.0778982\tvalid's binary_logloss: 0.19813\n","[130]\ttrain's binary_logloss: 0.0772169\tvalid's binary_logloss: 0.198024\n","[131]\ttrain's binary_logloss: 0.0766316\tvalid's binary_logloss: 0.198349\n","[132]\ttrain's binary_logloss: 0.0758333\tvalid's binary_logloss: 0.198234\n","[133]\ttrain's binary_logloss: 0.0752687\tvalid's binary_logloss: 0.197989\n","[134]\ttrain's binary_logloss: 0.0746069\tvalid's binary_logloss: 0.197879\n","[135]\ttrain's binary_logloss: 0.0739223\tvalid's binary_logloss: 0.197753\n","[136]\ttrain's binary_logloss: 0.0733035\tvalid's binary_logloss: 0.197642\n","[137]\ttrain's binary_logloss: 0.0726728\tvalid's binary_logloss: 0.197431\n","[138]\ttrain's binary_logloss: 0.071998\tvalid's binary_logloss: 0.19719\n","[139]\ttrain's binary_logloss: 0.0713678\tvalid's binary_logloss: 0.197065\n","[140]\ttrain's binary_logloss: 0.0707309\tvalid's binary_logloss: 0.197085\n","[141]\ttrain's binary_logloss: 0.0702072\tvalid's binary_logloss: 0.196979\n","[142]\ttrain's binary_logloss: 0.0696216\tvalid's binary_logloss: 0.19644\n","[143]\ttrain's binary_logloss: 0.0690718\tvalid's binary_logloss: 0.196351\n","[144]\ttrain's binary_logloss: 0.0685502\tvalid's binary_logloss: 0.19632\n","[145]\ttrain's binary_logloss: 0.0679334\tvalid's binary_logloss: 0.19625\n","[146]\ttrain's binary_logloss: 0.0672545\tvalid's binary_logloss: 0.196459\n","[147]\ttrain's binary_logloss: 0.0666915\tvalid's binary_logloss: 0.196416\n","[148]\ttrain's binary_logloss: 0.0661194\tvalid's binary_logloss: 0.196122\n","[149]\ttrain's binary_logloss: 0.0656072\tvalid's binary_logloss: 0.196111\n","[150]\ttrain's binary_logloss: 0.0649153\tvalid's binary_logloss: 0.195681\n","[151]\ttrain's binary_logloss: 0.0643105\tvalid's binary_logloss: 0.19581\n","[152]\ttrain's binary_logloss: 0.0638005\tvalid's binary_logloss: 0.195572\n","[153]\ttrain's binary_logloss: 0.0632577\tvalid's binary_logloss: 0.195658\n","[154]\ttrain's binary_logloss: 0.0626876\tvalid's binary_logloss: 0.195429\n","[155]\ttrain's binary_logloss: 0.062188\tvalid's binary_logloss: 0.195417\n","[156]\ttrain's binary_logloss: 0.0617055\tvalid's binary_logloss: 0.195548\n","[157]\ttrain's binary_logloss: 0.0611473\tvalid's binary_logloss: 0.195579\n","[158]\ttrain's binary_logloss: 0.0606703\tvalid's binary_logloss: 0.195664\n","[159]\ttrain's binary_logloss: 0.0601392\tvalid's binary_logloss: 0.195794\n","[160]\ttrain's binary_logloss: 0.0597131\tvalid's binary_logloss: 0.195683\n","[161]\ttrain's binary_logloss: 0.0592294\tvalid's binary_logloss: 0.19566\n","[162]\ttrain's binary_logloss: 0.0586743\tvalid's binary_logloss: 0.19508\n","[163]\ttrain's binary_logloss: 0.0582476\tvalid's binary_logloss: 0.195109\n","[164]\ttrain's binary_logloss: 0.0577289\tvalid's binary_logloss: 0.195071\n","[165]\ttrain's binary_logloss: 0.0573212\tvalid's binary_logloss: 0.194796\n","[166]\ttrain's binary_logloss: 0.0568503\tvalid's binary_logloss: 0.194893\n","[167]\ttrain's binary_logloss: 0.0564529\tvalid's binary_logloss: 0.194904\n","[168]\ttrain's binary_logloss: 0.0558647\tvalid's binary_logloss: 0.195069\n","[169]\ttrain's binary_logloss: 0.0553311\tvalid's binary_logloss: 0.195093\n","[170]\ttrain's binary_logloss: 0.0548367\tvalid's binary_logloss: 0.194723\n","[171]\ttrain's binary_logloss: 0.0543854\tvalid's binary_logloss: 0.194624\n","[172]\ttrain's binary_logloss: 0.0539162\tvalid's binary_logloss: 0.194629\n","[173]\ttrain's binary_logloss: 0.0534788\tvalid's binary_logloss: 0.19471\n","[174]\ttrain's binary_logloss: 0.053102\tvalid's binary_logloss: 0.19481\n","[175]\ttrain's binary_logloss: 0.0526168\tvalid's binary_logloss: 0.194615\n","[176]\ttrain's binary_logloss: 0.0522192\tvalid's binary_logloss: 0.194483\n","[177]\ttrain's binary_logloss: 0.0517949\tvalid's binary_logloss: 0.194482\n","[178]\ttrain's binary_logloss: 0.0514229\tvalid's binary_logloss: 0.194737\n","[179]\ttrain's binary_logloss: 0.0510057\tvalid's binary_logloss: 0.194741\n","[180]\ttrain's binary_logloss: 0.0506296\tvalid's binary_logloss: 0.194465\n","[181]\ttrain's binary_logloss: 0.0502384\tvalid's binary_logloss: 0.194523\n","[182]\ttrain's binary_logloss: 0.0498703\tvalid's binary_logloss: 0.194564\n","[183]\ttrain's binary_logloss: 0.0494347\tvalid's binary_logloss: 0.194271\n","[184]\ttrain's binary_logloss: 0.0489994\tvalid's binary_logloss: 0.194533\n","[185]\ttrain's binary_logloss: 0.0486505\tvalid's binary_logloss: 0.194815\n","[186]\ttrain's binary_logloss: 0.0482548\tvalid's binary_logloss: 0.195026\n","[187]\ttrain's binary_logloss: 0.0479446\tvalid's binary_logloss: 0.195132\n","[188]\ttrain's binary_logloss: 0.0475105\tvalid's binary_logloss: 0.194634\n","[189]\ttrain's binary_logloss: 0.0471474\tvalid's binary_logloss: 0.19449\n","[190]\ttrain's binary_logloss: 0.0466969\tvalid's binary_logloss: 0.194297\n","[191]\ttrain's binary_logloss: 0.0462442\tvalid's binary_logloss: 0.194411\n","[192]\ttrain's binary_logloss: 0.0458597\tvalid's binary_logloss: 0.194394\n","[193]\ttrain's binary_logloss: 0.0455464\tvalid's binary_logloss: 0.194468\n","[194]\ttrain's binary_logloss: 0.0450923\tvalid's binary_logloss: 0.19443\n","[195]\ttrain's binary_logloss: 0.044711\tvalid's binary_logloss: 0.194427\n","[196]\ttrain's binary_logloss: 0.0443407\tvalid's binary_logloss: 0.194434\n","[197]\ttrain's binary_logloss: 0.0439693\tvalid's binary_logloss: 0.194523\n","[198]\ttrain's binary_logloss: 0.0436881\tvalid's binary_logloss: 0.194706\n","[199]\ttrain's binary_logloss: 0.043264\tvalid's binary_logloss: 0.194515\n","[200]\ttrain's binary_logloss: 0.0429223\tvalid's binary_logloss: 0.19433\n","[201]\ttrain's binary_logloss: 0.0425909\tvalid's binary_logloss: 0.194262\n","[202]\ttrain's binary_logloss: 0.0422625\tvalid's binary_logloss: 0.194283\n","[203]\ttrain's binary_logloss: 0.0419056\tvalid's binary_logloss: 0.194091\n","[204]\ttrain's binary_logloss: 0.0416149\tvalid's binary_logloss: 0.194175\n","[205]\ttrain's binary_logloss: 0.041324\tvalid's binary_logloss: 0.194223\n","[206]\ttrain's binary_logloss: 0.0410348\tvalid's binary_logloss: 0.194177\n","[207]\ttrain's binary_logloss: 0.0406937\tvalid's binary_logloss: 0.194167\n","[208]\ttrain's binary_logloss: 0.04038\tvalid's binary_logloss: 0.194005\n","[209]\ttrain's binary_logloss: 0.0400649\tvalid's binary_logloss: 0.193983\n","[210]\ttrain's binary_logloss: 0.0397268\tvalid's binary_logloss: 0.193964\n","[211]\ttrain's binary_logloss: 0.0394542\tvalid's binary_logloss: 0.193827\n","[212]\ttrain's binary_logloss: 0.0391675\tvalid's binary_logloss: 0.193478\n","[213]\ttrain's binary_logloss: 0.0389026\tvalid's binary_logloss: 0.193396\n","[214]\ttrain's binary_logloss: 0.0385554\tvalid's binary_logloss: 0.193739\n","[215]\ttrain's binary_logloss: 0.0382692\tvalid's binary_logloss: 0.194168\n","[216]\ttrain's binary_logloss: 0.0379723\tvalid's binary_logloss: 0.194109\n","[217]\ttrain's binary_logloss: 0.0376989\tvalid's binary_logloss: 0.193965\n","[218]\ttrain's binary_logloss: 0.0374031\tvalid's binary_logloss: 0.194076\n","[219]\ttrain's binary_logloss: 0.0371135\tvalid's binary_logloss: 0.194399\n","[220]\ttrain's binary_logloss: 0.0368469\tvalid's binary_logloss: 0.194598\n","[221]\ttrain's binary_logloss: 0.0365554\tvalid's binary_logloss: 0.194517\n","[222]\ttrain's binary_logloss: 0.036294\tvalid's binary_logloss: 0.194431\n","[223]\ttrain's binary_logloss: 0.0360607\tvalid's binary_logloss: 0.194292\n","[224]\ttrain's binary_logloss: 0.0358115\tvalid's binary_logloss: 0.194377\n","[225]\ttrain's binary_logloss: 0.0355769\tvalid's binary_logloss: 0.194259\n","[226]\ttrain's binary_logloss: 0.0352559\tvalid's binary_logloss: 0.194174\n","[227]\ttrain's binary_logloss: 0.0349577\tvalid's binary_logloss: 0.194401\n","[228]\ttrain's binary_logloss: 0.0346354\tvalid's binary_logloss: 0.194084\n","[229]\ttrain's binary_logloss: 0.0344195\tvalid's binary_logloss: 0.194048\n","[230]\ttrain's binary_logloss: 0.0341561\tvalid's binary_logloss: 0.194388\n","[231]\ttrain's binary_logloss: 0.0338852\tvalid's binary_logloss: 0.194557\n","[232]\ttrain's binary_logloss: 0.0336674\tvalid's binary_logloss: 0.194634\n","[233]\ttrain's binary_logloss: 0.0334315\tvalid's binary_logloss: 0.19492\n","Early stopping, best iteration is:\n","[213]\ttrain's binary_logloss: 0.0389026\tvalid's binary_logloss: 0.193396\n","[1]\ttrain's binary_logloss: 0.454002\tvalid's binary_logloss: 0.463717\n","Training until validation scores don't improve for 20 rounds.\n","[2]\ttrain's binary_logloss: 0.428562\tvalid's binary_logloss: 0.440947\n","[3]\ttrain's binary_logloss: 0.407599\tvalid's binary_logloss: 0.422614\n","[4]\ttrain's binary_logloss: 0.390282\tvalid's binary_logloss: 0.407262\n","[5]\ttrain's binary_logloss: 0.375377\tvalid's binary_logloss: 0.395027\n","[6]\ttrain's binary_logloss: 0.362372\tvalid's binary_logloss: 0.383689\n","[7]\ttrain's binary_logloss: 0.35164\tvalid's binary_logloss: 0.37506\n","[8]\ttrain's binary_logloss: 0.340887\tvalid's binary_logloss: 0.365699\n","[9]\ttrain's binary_logloss: 0.330988\tvalid's binary_logloss: 0.357268\n","[10]\ttrain's binary_logloss: 0.322747\tvalid's binary_logloss: 0.351206\n","[11]\ttrain's binary_logloss: 0.314685\tvalid's binary_logloss: 0.343512\n","[12]\ttrain's binary_logloss: 0.307382\tvalid's binary_logloss: 0.337971\n","[13]\ttrain's binary_logloss: 0.299469\tvalid's binary_logloss: 0.331879\n","[14]\ttrain's binary_logloss: 0.293242\tvalid's binary_logloss: 0.326949\n","[15]\ttrain's binary_logloss: 0.287616\tvalid's binary_logloss: 0.322713\n","[16]\ttrain's binary_logloss: 0.281928\tvalid's binary_logloss: 0.319091\n","[17]\ttrain's binary_logloss: 0.275859\tvalid's binary_logloss: 0.314519\n","[18]\ttrain's binary_logloss: 0.270841\tvalid's binary_logloss: 0.311057\n","[19]\ttrain's binary_logloss: 0.266028\tvalid's binary_logloss: 0.30741\n","[20]\ttrain's binary_logloss: 0.259543\tvalid's binary_logloss: 0.303093\n","[21]\ttrain's binary_logloss: 0.254906\tvalid's binary_logloss: 0.300208\n","[22]\ttrain's binary_logloss: 0.249399\tvalid's binary_logloss: 0.29631\n","[23]\ttrain's binary_logloss: 0.244523\tvalid's binary_logloss: 0.292901\n","[24]\ttrain's binary_logloss: 0.239795\tvalid's binary_logloss: 0.289645\n","[25]\ttrain's binary_logloss: 0.236157\tvalid's binary_logloss: 0.286903\n","[26]\ttrain's binary_logloss: 0.23234\tvalid's binary_logloss: 0.285204\n","[27]\ttrain's binary_logloss: 0.227961\tvalid's binary_logloss: 0.282611\n","[28]\ttrain's binary_logloss: 0.224817\tvalid's binary_logloss: 0.280282\n","[29]\ttrain's binary_logloss: 0.220883\tvalid's binary_logloss: 0.278424\n","[30]\ttrain's binary_logloss: 0.217562\tvalid's binary_logloss: 0.276957\n","[31]\ttrain's binary_logloss: 0.214455\tvalid's binary_logloss: 0.27411\n","[32]\ttrain's binary_logloss: 0.211108\tvalid's binary_logloss: 0.272863\n","[33]\ttrain's binary_logloss: 0.208011\tvalid's binary_logloss: 0.271114\n","[34]\ttrain's binary_logloss: 0.205489\tvalid's binary_logloss: 0.26993\n","[35]\ttrain's binary_logloss: 0.202929\tvalid's binary_logloss: 0.268422\n","[36]\ttrain's binary_logloss: 0.199286\tvalid's binary_logloss: 0.266072\n","[37]\ttrain's binary_logloss: 0.196455\tvalid's binary_logloss: 0.264678\n","[38]\ttrain's binary_logloss: 0.193871\tvalid's binary_logloss: 0.26313\n","[39]\ttrain's binary_logloss: 0.191135\tvalid's binary_logloss: 0.261711\n","[40]\ttrain's binary_logloss: 0.188875\tvalid's binary_logloss: 0.259951\n","[41]\ttrain's binary_logloss: 0.186568\tvalid's binary_logloss: 0.259021\n","[42]\ttrain's binary_logloss: 0.184058\tvalid's binary_logloss: 0.257889\n","[43]\ttrain's binary_logloss: 0.181375\tvalid's binary_logloss: 0.256299\n","[44]\ttrain's binary_logloss: 0.179181\tvalid's binary_logloss: 0.25533\n","[45]\ttrain's binary_logloss: 0.17657\tvalid's binary_logloss: 0.254815\n","[46]\ttrain's binary_logloss: 0.174711\tvalid's binary_logloss: 0.253911\n","[47]\ttrain's binary_logloss: 0.172531\tvalid's binary_logloss: 0.253273\n","[48]\ttrain's binary_logloss: 0.170531\tvalid's binary_logloss: 0.251909\n","[49]\ttrain's binary_logloss: 0.168491\tvalid's binary_logloss: 0.250985\n","[50]\ttrain's binary_logloss: 0.165977\tvalid's binary_logloss: 0.249564\n","[51]\ttrain's binary_logloss: 0.164234\tvalid's binary_logloss: 0.249239\n","[52]\ttrain's binary_logloss: 0.161815\tvalid's binary_logloss: 0.247996\n","[53]\ttrain's binary_logloss: 0.160238\tvalid's binary_logloss: 0.247119\n","[54]\ttrain's binary_logloss: 0.158453\tvalid's binary_logloss: 0.246054\n","[55]\ttrain's binary_logloss: 0.156827\tvalid's binary_logloss: 0.245713\n","[56]\ttrain's binary_logloss: 0.154813\tvalid's binary_logloss: 0.244461\n","[57]\ttrain's binary_logloss: 0.15308\tvalid's binary_logloss: 0.243816\n","[58]\ttrain's binary_logloss: 0.151192\tvalid's binary_logloss: 0.243238\n","[59]\ttrain's binary_logloss: 0.149379\tvalid's binary_logloss: 0.242223\n","[60]\ttrain's binary_logloss: 0.147302\tvalid's binary_logloss: 0.241406\n","[61]\ttrain's binary_logloss: 0.145758\tvalid's binary_logloss: 0.241085\n","[62]\ttrain's binary_logloss: 0.144182\tvalid's binary_logloss: 0.240885\n","[63]\ttrain's binary_logloss: 0.14289\tvalid's binary_logloss: 0.240444\n","[64]\ttrain's binary_logloss: 0.141317\tvalid's binary_logloss: 0.239756\n","[65]\ttrain's binary_logloss: 0.139554\tvalid's binary_logloss: 0.238521\n","[66]\ttrain's binary_logloss: 0.13809\tvalid's binary_logloss: 0.237868\n","[67]\ttrain's binary_logloss: 0.136666\tvalid's binary_logloss: 0.237348\n","[68]\ttrain's binary_logloss: 0.135253\tvalid's binary_logloss: 0.237173\n","[69]\ttrain's binary_logloss: 0.133852\tvalid's binary_logloss: 0.23631\n","[70]\ttrain's binary_logloss: 0.132582\tvalid's binary_logloss: 0.23586\n","[71]\ttrain's binary_logloss: 0.131338\tvalid's binary_logloss: 0.235464\n","[72]\ttrain's binary_logloss: 0.129755\tvalid's binary_logloss: 0.234707\n","[73]\ttrain's binary_logloss: 0.128287\tvalid's binary_logloss: 0.233781\n","[74]\ttrain's binary_logloss: 0.12702\tvalid's binary_logloss: 0.233736\n","[75]\ttrain's binary_logloss: 0.125821\tvalid's binary_logloss: 0.233247\n","[76]\ttrain's binary_logloss: 0.124549\tvalid's binary_logloss: 0.232647\n","[77]\ttrain's binary_logloss: 0.123014\tvalid's binary_logloss: 0.231895\n","[78]\ttrain's binary_logloss: 0.121895\tvalid's binary_logloss: 0.232004\n","[79]\ttrain's binary_logloss: 0.12075\tvalid's binary_logloss: 0.231333\n","[80]\ttrain's binary_logloss: 0.119603\tvalid's binary_logloss: 0.230905\n","[81]\ttrain's binary_logloss: 0.118411\tvalid's binary_logloss: 0.230839\n","[82]\ttrain's binary_logloss: 0.11718\tvalid's binary_logloss: 0.230068\n","[83]\ttrain's binary_logloss: 0.116038\tvalid's binary_logloss: 0.22993\n","[84]\ttrain's binary_logloss: 0.11454\tvalid's binary_logloss: 0.229423\n","[85]\ttrain's binary_logloss: 0.113205\tvalid's binary_logloss: 0.228402\n","[86]\ttrain's binary_logloss: 0.112145\tvalid's binary_logloss: 0.228431\n","[87]\ttrain's binary_logloss: 0.111142\tvalid's binary_logloss: 0.228003\n","[88]\ttrain's binary_logloss: 0.110108\tvalid's binary_logloss: 0.227613\n","[89]\ttrain's binary_logloss: 0.109058\tvalid's binary_logloss: 0.227482\n","[90]\ttrain's binary_logloss: 0.108082\tvalid's binary_logloss: 0.227313\n","[91]\ttrain's binary_logloss: 0.106887\tvalid's binary_logloss: 0.226931\n","[92]\ttrain's binary_logloss: 0.106037\tvalid's binary_logloss: 0.226633\n","[93]\ttrain's binary_logloss: 0.105101\tvalid's binary_logloss: 0.226467\n","[94]\ttrain's binary_logloss: 0.104193\tvalid's binary_logloss: 0.226735\n","[95]\ttrain's binary_logloss: 0.103346\tvalid's binary_logloss: 0.226747\n","[96]\ttrain's binary_logloss: 0.102144\tvalid's binary_logloss: 0.226233\n","[97]\ttrain's binary_logloss: 0.101185\tvalid's binary_logloss: 0.225848\n","[98]\ttrain's binary_logloss: 0.100395\tvalid's binary_logloss: 0.225934\n","[99]\ttrain's binary_logloss: 0.0993901\tvalid's binary_logloss: 0.225952\n","[100]\ttrain's binary_logloss: 0.0983998\tvalid's binary_logloss: 0.225351\n","[101]\ttrain's binary_logloss: 0.0974316\tvalid's binary_logloss: 0.225012\n","[102]\ttrain's binary_logloss: 0.0964874\tvalid's binary_logloss: 0.224871\n","[103]\ttrain's binary_logloss: 0.0956984\tvalid's binary_logloss: 0.224808\n","[104]\ttrain's binary_logloss: 0.0949481\tvalid's binary_logloss: 0.224409\n","[105]\ttrain's binary_logloss: 0.0939826\tvalid's binary_logloss: 0.224027\n","[106]\ttrain's binary_logloss: 0.0931023\tvalid's binary_logloss: 0.223932\n","[107]\ttrain's binary_logloss: 0.0921883\tvalid's binary_logloss: 0.223605\n","[108]\ttrain's binary_logloss: 0.0914231\tvalid's binary_logloss: 0.223656\n","[109]\ttrain's binary_logloss: 0.0905756\tvalid's binary_logloss: 0.223152\n","[110]\ttrain's binary_logloss: 0.0898809\tvalid's binary_logloss: 0.222858\n","[111]\ttrain's binary_logloss: 0.0890632\tvalid's binary_logloss: 0.22285\n","[112]\ttrain's binary_logloss: 0.0882689\tvalid's binary_logloss: 0.222405\n","[113]\ttrain's binary_logloss: 0.0872993\tvalid's binary_logloss: 0.222077\n","[114]\ttrain's binary_logloss: 0.08632\tvalid's binary_logloss: 0.221866\n","[115]\ttrain's binary_logloss: 0.0857026\tvalid's binary_logloss: 0.221718\n","[116]\ttrain's binary_logloss: 0.084662\tvalid's binary_logloss: 0.220962\n","[117]\ttrain's binary_logloss: 0.0838724\tvalid's binary_logloss: 0.221021\n","[118]\ttrain's binary_logloss: 0.0831336\tvalid's binary_logloss: 0.22072\n","[119]\ttrain's binary_logloss: 0.0823967\tvalid's binary_logloss: 0.220684\n","[120]\ttrain's binary_logloss: 0.0816005\tvalid's binary_logloss: 0.220695\n","[121]\ttrain's binary_logloss: 0.0808314\tvalid's binary_logloss: 0.220622\n","[122]\ttrain's binary_logloss: 0.0802602\tvalid's binary_logloss: 0.22051\n","[123]\ttrain's binary_logloss: 0.0793987\tvalid's binary_logloss: 0.220277\n","[124]\ttrain's binary_logloss: 0.0786381\tvalid's binary_logloss: 0.220329\n","[125]\ttrain's binary_logloss: 0.0780358\tvalid's binary_logloss: 0.2201\n","[126]\ttrain's binary_logloss: 0.0773525\tvalid's binary_logloss: 0.220171\n","[127]\ttrain's binary_logloss: 0.0766694\tvalid's binary_logloss: 0.220137\n","[128]\ttrain's binary_logloss: 0.0760041\tvalid's binary_logloss: 0.22029\n","[129]\ttrain's binary_logloss: 0.0752733\tvalid's binary_logloss: 0.21987\n","[130]\ttrain's binary_logloss: 0.0746004\tvalid's binary_logloss: 0.219843\n","[131]\ttrain's binary_logloss: 0.0740644\tvalid's binary_logloss: 0.219797\n","[132]\ttrain's binary_logloss: 0.073308\tvalid's binary_logloss: 0.219097\n","[133]\ttrain's binary_logloss: 0.0727453\tvalid's binary_logloss: 0.219058\n","[134]\ttrain's binary_logloss: 0.0719931\tvalid's binary_logloss: 0.218696\n","[135]\ttrain's binary_logloss: 0.0714457\tvalid's binary_logloss: 0.218734\n","[136]\ttrain's binary_logloss: 0.0708093\tvalid's binary_logloss: 0.218829\n","[137]\ttrain's binary_logloss: 0.0702374\tvalid's binary_logloss: 0.218468\n","[138]\ttrain's binary_logloss: 0.0696918\tvalid's binary_logloss: 0.218615\n","[139]\ttrain's binary_logloss: 0.0691296\tvalid's binary_logloss: 0.218932\n","[140]\ttrain's binary_logloss: 0.068597\tvalid's binary_logloss: 0.218926\n","[141]\ttrain's binary_logloss: 0.0680037\tvalid's binary_logloss: 0.218786\n","[142]\ttrain's binary_logloss: 0.0674294\tvalid's binary_logloss: 0.218626\n","[143]\ttrain's binary_logloss: 0.0668463\tvalid's binary_logloss: 0.218499\n","[144]\ttrain's binary_logloss: 0.0662968\tvalid's binary_logloss: 0.21892\n","[145]\ttrain's binary_logloss: 0.0657743\tvalid's binary_logloss: 0.219062\n","[146]\ttrain's binary_logloss: 0.0651522\tvalid's binary_logloss: 0.218886\n","[147]\ttrain's binary_logloss: 0.0646621\tvalid's binary_logloss: 0.218776\n","[148]\ttrain's binary_logloss: 0.0642182\tvalid's binary_logloss: 0.218785\n","[149]\ttrain's binary_logloss: 0.063697\tvalid's binary_logloss: 0.218751\n","[150]\ttrain's binary_logloss: 0.0632319\tvalid's binary_logloss: 0.218765\n","[151]\ttrain's binary_logloss: 0.0627609\tvalid's binary_logloss: 0.218658\n","[152]\ttrain's binary_logloss: 0.0621993\tvalid's binary_logloss: 0.218729\n","[153]\ttrain's binary_logloss: 0.061768\tvalid's binary_logloss: 0.218904\n","[154]\ttrain's binary_logloss: 0.0612907\tvalid's binary_logloss: 0.219143\n","[155]\ttrain's binary_logloss: 0.0607988\tvalid's binary_logloss: 0.21898\n","[156]\ttrain's binary_logloss: 0.060162\tvalid's binary_logloss: 0.218763\n","[157]\ttrain's binary_logloss: 0.0595107\tvalid's binary_logloss: 0.218477\n","Early stopping, best iteration is:\n","[137]\ttrain's binary_logloss: 0.0702374\tvalid's binary_logloss: 0.218468\n","0.21846813593111647\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SQYsmxlXILrh"},"source":["## **stratified k-fold**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"b6S-y-3QImO6"},"source":["- 分類タスクの場合に, foldごとに含まれるクラスの割合を等しくすることがしばしば行われ, これを層化抽出 (stratified sampling) と呼ぶ. \n","\n","- テストデータに含まれる各クラスの割合は, 学習データに含まれる各クラスの割合とほぼ同じであろうという仮定に基づき, バリデーションの評価を安定させようとする手法.\n","\n","- 特に多クラス分類で極端に頻度の少ないクラスがある際は, ランダムに分割した場合には各クラスの割合にむらが生じ, 評価のぶれが大きくなる可能性があるため, 層化抽出を行うことが重要.\n","\n","- 基本的に, 分類問題の場合は層化抽出を行う方が良い.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ztsryuqyHdg5"},"source":["from sklearn.model_selection import StratifiedKFold\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train_preprocessed.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test_preprocessed.csv')\n","\n","kf = StratifiedKFold(n_splits=4, shuffle=True, random_state=71)\n","for tr_idx, va_idx in kf.split(train_x, train_y):\n","  tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n","  tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"74Goag_HUvX8"},"source":["## **group k-fold**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"mgg_TZ6hUyn2"},"source":["- 学習データとテストデータがランダムに分割されていない場合に, バリデーションにおいても同様の条件とするため, 学習データをグループ化して学習用データとバリデーションデータへの分割を行うこと.\n","\n","- 学習データとテストデータに同一単位のデータが混在しないように分割されることがよくある. これは, 別単位のデータのみを使って新たな単位の予測を行う状況を想定している.\n"," - 他の顧客のデータのみを使って新たな顧客の予測を行う状況などが挙げられる.\n"," - この場合にランダムにデータを分割してバリデーションを行ってしまうと, 本来の性能よりも過大評価してしまう恐れがある. バリデーションデータに存在する顧客のデータが学習データに含まれることで, 本来知り得ない情報を知ることができ, 予測しやするなるからである. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"f0pu3qdlKWvM"},"source":["from sklearn.model_selection import KFold\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train_preprocessed.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test_preprocessed.csv')\n","\n","train_x['user_id'] = np.arange(0, len(train_x)) // 4\n","user_id = train_x['user_id']\n","unique_user_ids = user_id.unique()\n","\n","kf = KFold(n_splits=4, shuffle=True, random_state=71)\n","for tr_group_idx, va_group_idx in kf.split(unique_user_ids):\n","  tr_groups, va_groups = unique_user_ids[tr_group_idx], unique_user_ids[va_group_idx]\n","\n","  is_tr, is_va = user_id.isin(tr_groups), user_id.isin(va_groups)\n","  tr_x, va_x = train_x[is_tr], train_x[is_va]\n","  tr_y, va_y = train_y[is_tr], train_y[is_va]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9w3kTTsIh6O-"},"source":["## **leave-one-out**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"x7-w_sA5h6U2"},"source":["- クロスバリデーションにおいて, fold数を学習データのレコード数と同じにし, バリデーションデータをそれぞれ1件とする手法. \n","\n","- leave-one-outのようなfold数が大きい, すなわち各foldのバリデーションデータが少ないクロスバリデーションにおいて, アーリーストッピングを用いると, バリデーションデータへの適合が強くなり, モデルの精度が過大評価される問題が顕著になる.\n"," - 対処法の1つとしては, 一度各foldでアーリーストッピングを行い, その平均などで適切なイテレーション数を見積もった後に, イテレーション数を固定して再度クロスバリデーションを行う方法が考えられる."]},{"cell_type":"code","metadata":{"id":"FpNxhr6fccZa"},"source":["\n","from sklearn.model_selection import KFold\n","\n","train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/train_preprocessed.csv')\n","train_x = train.drop(['target'], axis=1)\n","train_y = train['target']\n","test_x = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/input/test_preprocessed.csv')\n","\n","kf = KFold(n_splits=len(train_x), shuffle=True, random_state=71)\n","for tr_idx, va_idx in kf.split(train_x):\n","  tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n","  tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLyNRfnx4OST"},"source":["# **バリデーションのポイントとテクニック**"]},{"cell_type":"markdown","metadata":{"id":"KtbE45F14OeE"},"source":["## **バリデーションを行う目的**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"GJJ7-JzY510j"},"source":["- モデルを改善していく上での指針となるスコアを示す.\n"," - 正しくバリデーションが出来ていない場合には,　誤った方向にモデルの修正を進めてしまう.\n","- テストデータに対するスコアやそのばらつきを見積もる."]},{"cell_type":"markdown","metadata":{"id":"8TYHe7Rd5lq8"},"source":["## **学習データとテストデータの分割をまねる**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"EBmZGjQT53aE"},"source":["- どのようなバリデーションを行うべきか迷った際には, 学習データとテストデータの分割をまねるという考え方が有効.\n","\n","- データの分割が典型的な場合には, 主なバリデーション手法にあてはまることが多い.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sdbH_GQX_vjO"},"source":["## **学習データとテストデータの分布が違う場合**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"K6BL9YBt_1Pl"},"source":["- 学習データとテストデータの傾向の違いについて, データの作成過程やEDAを基に考察する.\n","\n","- モデルを複雑にしすぎないことや効く理由が説明できる特徴量を使うことで, 分布の違いに頑強な予測にする.\n","\n","- さまざまなモデルの平均をとるアンサンブルによって予測を安定させ, 分布の違いに頑強な予測にする.\n","\n","- adversarial validationの結果を参考にして, 適切なバリデーション方法を確立する. \n","\n","- adversarial validationのスコアが低くなるように特徴量を変換することで, 分布の違いに影響されづらい予測とする.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zowUR0qzB0B9"},"source":["### **adversarial validation**\n","\n","- 学習データとテストデータを結合し, テストデータか否かを目的変数とする二値分類を行うことで, 学習データとテストデータの分布が同じかどうかを判断することができる.\n"," - 同じ分布であれば, それらの見分けはできないので, その二値分類でのAUCは0.5に近くなる. 一方, AUCが1に近くなった場合は, テストデータか否かを見分けられる情報があることになる.\n","\n","- AUCが0.5を十分上回るような, 学習データとテストデータが異なる分布の場合に, テストデータに近い学習データをバリデーションデータとすることで, テストデータを上手く模倣したデータでの評価が期待できる.　このバリデーション手法をadversarial validationと呼ぶ.\n"," - 特徴量の作成などはせず, 与えられたデータをそのまま結合して入力データとするのが良い.\n","\n","1. 学習データとテストデータを結合し, テストデータか否かを目的変数とする二値分類を行うモデルを作成する.\n","1. それぞれのレコードがテストデータである確率の予測値を出力する.\n","1. テストデータである確率が高いと予測された学習データを一定数選んでバリデーションデータとする.\n","1. 選んだバリデーションデータで, 本来のタスクのバリデーションを行う."]},{"cell_type":"code","metadata":{"id":"pYdJWq5Wi3l1"},"source":[""],"execution_count":null,"outputs":[]}]}